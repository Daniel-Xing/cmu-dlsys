# æ·±åº¦å­¦ä¹ ç³»ç»Ÿä»Žå…¥é—¨åˆ°æ”¾å¼ƒ - CMU-DL System Lab1-1 å‰å‘è®¡ç®—å’Œæ¢¯åº¦ä¼ æ’­

lab1å¼€å§‹æ­£å¼è¿›å…¥åˆ°æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ¡†æž¶çš„å¼€å‘ï¼Œlab1å°†å¸®åŠ©ä½ å¼€å§‹å®žçŽ°ä¸€ä¸ªå« **needle** ï¼ˆ**ne**cessary **e**lements of **d**eep **le**arningï¼‰åº“ã€‚å…·ä½“æ¥è¯´ï¼Œlab1çš„ç›®æ ‡æ˜¯æž„å»ºä¸€ä¸ªåŸºæœ¬çš„**è‡ªåŠ¨å¾®åˆ†**æ¡†æž¶ï¼Œç„¶åŽä½¿ç”¨å®ƒæ¥é‡æ–°å®žçŽ°HW0ä¸­ç”¨äºŽMNISTæ•°å­—åˆ†ç±»é—®é¢˜çš„ç®€å•åŒå±‚ç¥žç»ç½‘ç»œã€‚

# ä»€ä¹ˆæ˜¯Neddle

`needle`æ˜¯ä¸€ä¸ªåŸºäºŽ `numpy` CPUåŽç«¯çš„è‡ªåŠ¨å¾®åˆ†åº“ï¼Œå°†åœ¨è¯¾ç¨‹ä¸­é€æ­¥æ‰©å±•åˆ°åŒ…å«GPUä»£ç çš„çº¿æ€§ä»£æ•°åº“ã€‚è¿™æ¬¡ä½œä¸šä¸­ï¼Œä½ å°†ä½¿ç”¨Pythonè¯­è¨€æ¥å®žçŽ°è‡ªåŠ¨å¾®åˆ†çš„åŸºç¡€ã€‚

åœ¨ `needle`åº“ä¸­ï¼Œæœ‰ä¸¤ä¸ªé‡è¦çš„æ–‡ä»¶ï¼š`python/needle/autograd.py`ï¼ˆå®šä¹‰äº†è®¡ç®—å›¾æ¡†æž¶çš„åŸºç¡€ï¼Œå¹¶å°†æˆä¸ºè‡ªåŠ¨å¾®åˆ†æ¡†æž¶çš„åŸºç¡€ï¼‰å’Œ `python/needle/ops/ops_mathematic.py`ï¼ˆåŒ…å«å„ç§è¿ç®—ç¬¦çš„å®žçŽ°ï¼Œä½ å°†åœ¨ä½œä¸šå’Œè¯¾ç¨‹ä¸­ä½¿ç”¨è¿™äº›è¿ç®—ç¬¦ï¼‰ã€‚

è™½ç„¶ `autograd.py`æ–‡ä»¶å·²ç»å»ºç«‹äº†è‡ªåŠ¨å¾®åˆ†çš„åŸºæœ¬æ¡†æž¶ï¼Œä½†ä½ åº”è¯¥ç†Ÿæ‚‰åº“çš„åŸºæœ¬æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯ä»¥ä¸‹å‡ ä¸ªå®šä¹‰çš„ç±»ï¼š

- `Value`ï¼šåœ¨è®¡ç®—å›¾ä¸­è®¡ç®—å‡ºçš„å€¼ï¼Œå¯ä»¥æ˜¯å¯¹å…¶ä»– `Value`å¯¹è±¡åº”ç”¨çš„æ“ä½œçš„è¾“å‡ºï¼Œæˆ–è€…æ˜¯å¸¸æ•°ï¼ˆå¶å­ï¼‰`Value`å¯¹è±¡ã€‚è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªé€šç”¨ç±»ï¼ˆç„¶åŽä¸“é—¨ç”¨äºŽä¾‹å¦‚å¼ é‡ï¼‰ï¼Œä»¥ä¾¿äºŽä»¥åŽç‰ˆæœ¬çš„needleä¸­ä½¿ç”¨å…¶ä»–æ•°æ®ç»“æž„ï¼Œä½†ç›®å‰ä½ ä¸»è¦é€šè¿‡å®ƒçš„å­ç±» `Tensor`ï¼ˆè§ä¸‹æ–‡ï¼‰ä¸Žè¿™ä¸ªç±»äº¤äº’ã€‚
- `Op`ï¼šè®¡ç®—å›¾ä¸­çš„ä¸€ä¸ªè¿ç®—ç¬¦ã€‚è¿ç®—ç¬¦éœ€è¦åœ¨ `compute()`æ–¹æ³•ä¸­å®šä¹‰å®ƒä»¬çš„â€œå‰å‘â€è¿‡ç¨‹ï¼ˆå³å¦‚ä½•åœ¨ `Value`å¯¹è±¡çš„åº•å±‚æ•°æ®ä¸Šè®¡ç®—è¿ç®—ç¬¦ï¼‰ï¼Œä»¥åŠé€šè¿‡ `gradient()`æ–¹æ³•å®šä¹‰å®ƒä»¬çš„â€œåå‘â€è¿‡ç¨‹ï¼Œå³å¦‚ä½•ä¹˜ä»¥ä¼ å…¥çš„è¾“å‡ºæ¢¯åº¦ã€‚å¦‚ä½•ç¼–å†™è¿™æ ·çš„è¿ç®—ç¬¦çš„ç»†èŠ‚å°†åœ¨ä¸‹æ–‡ä¸­ç»™å‡ºã€‚
- `Tensor`ï¼š`Value`çš„ä¸€ä¸ªå­ç±»ï¼Œå¯¹åº”äºŽè®¡ç®—å›¾ä¸­çš„å®žé™…å¼ é‡è¾“å‡ºï¼Œå³å¤šç»´æ•°ç»„ã€‚ä½ åœ¨è¿™æ¬¡ä½œä¸šä¸­çš„æ‰€æœ‰ä»£ç ï¼ˆä»¥åŠå¤§å¤šæ•°åŽç»­ä½œä¸šï¼‰éƒ½å°†ä½¿ç”¨è¿™ä¸ª `Value`çš„å­ç±»è€Œä¸æ˜¯ä¸Šé¢çš„é€šç”¨ç±»ã€‚æˆ‘ä»¬æä¾›äº†å‡ ä¸ªä¾¿åˆ©å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œæ“ä½œç¬¦é‡è½½ï¼‰ï¼Œè®©ä½ èƒ½å¤Ÿä½¿ç”¨æ­£å¸¸çš„Pythonæƒ¯ä¾‹æ“ä½œå¼ é‡ï¼Œä½†è¿™äº›å‡½æ•°åœ¨ä½ å®žçŽ°ç›¸åº”æ“ä½œä¹‹å‰å°†ä¸ä¼šæ­£å¸¸å·¥ä½œã€‚
- `TensorOp`ï¼šæ˜¯ `Op`çš„ä¸€ä¸ªå­ç±»ï¼Œç”¨äºŽè¿”å›žå¼ é‡çš„è¿ç®—ç¬¦ã€‚ä½ åœ¨è¿™æ¬¡ä½œä¸šä¸­å®žçŽ°çš„æ‰€æœ‰æ“ä½œéƒ½å°†æ˜¯è¿™ç§ç±»åž‹ã€‚

# é—®é¢˜1ï¼šå®žçŽ°å‰å‘è®¡ç®—

é—®é¢˜1ä¸­ï¼Œæˆ‘ä»¬å°†ä¸ºå¤šä¸ªç±»å®žçŽ° `compute`æ–¹æ³•ä»¥è¿›è¡Œå‰å‘è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œåœ¨ `ops/ops_mathematic.py`æ–‡ä»¶ä¸­çš„ `EWiseAdd`è¿ç®—ç¬¦ï¼Œå®ƒçš„ `compute()`å‡½æ•°æ‰§è¡Œçš„æ˜¯å‰å‘ä¼ æ’­è®¡ç®—ï¼Œå³ç›´æŽ¥è®¡ç®—æ“ä½œæœ¬èº«ï¼Œè¾“å…¥ä¸º `NDArray`å¯¹è±¡ã€‚è€Œ `gradient()`å‡½æ•°è´Ÿè´£è®¡ç®—æ¢¯åº¦ï¼Œå…¶å‚æ•°ä¸º `Tensor`å¯¹è±¡ï¼Œæ„å‘³ç€å‡½æ•°å†…éƒ¨çš„ä»»ä½•è°ƒç”¨éƒ½åº”è¯¥é€šè¿‡ `TensorOp`è¿ç®—æ¥å®Œæˆã€‚å¦å¤–ï¼Œä¸ºäº†ç®€åŒ–æ“ä½œï¼Œå®šä¹‰äº†è¾…åŠ©å‡½æ•° `add()`ï¼Œä»¥ä¾¿æ›´ç®€æ´åœ°å®žçŽ°ä¸¤ä¸ª `Tensor`å¯¹è±¡çš„ç›¸åŠ ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬éœ€è¦å®žçŽ°çš„è¿ç®—ç¬¦çš„ `compute`æ–¹æ³•åˆ—è¡¨ï¼š

- `PowerScalar`ï¼šå°†è¾“å…¥æå‡è‡³æ•´æ•°ï¼ˆæ ‡é‡ï¼‰å¹‚çº§ã€‚
- `EWiseDiv`ï¼šå¯¹è¾“å…¥è¿›è¡Œé€å…ƒç´ çš„çœŸé™¤æ³•ï¼ˆ2ä¸ªè¾“å…¥ï¼‰ã€‚
- `DivScalar`ï¼šå°†è¾“å…¥é€å…ƒç´ åœ°é™¤ä»¥ä¸€ä¸ªæ ‡é‡ï¼ˆ1ä¸ªè¾“å…¥ï¼Œ`scalar` - æ•°å­—ï¼‰ã€‚
- `MatMul`ï¼šå¯¹è¾“å…¥è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼ˆ2ä¸ªè¾“å…¥ï¼‰ã€‚
- `Summation`ï¼šåœ¨æŒ‡å®šè½´ä¸Šå¯¹æ•°ç»„å…ƒç´ æ±‚å’Œï¼ˆ1ä¸ªè¾“å…¥ï¼Œ`axes` - å…ƒç»„ï¼‰ã€‚
- `BroadcastTo`ï¼šå°†æ•°ç»„å¹¿æ’­åˆ°æ–°çš„å½¢çŠ¶ï¼ˆ1ä¸ªè¾“å…¥ï¼Œ`shape` - å…ƒç»„ï¼‰ã€‚
- `Reshape`ï¼šä¸æ”¹å˜æ•°æ®çš„å‰æä¸‹ï¼Œä¸ºæ•°ç»„æä¾›æ–°çš„å½¢çŠ¶ï¼ˆ1ä¸ªè¾“å…¥ï¼Œ`shape` - å…ƒç»„ï¼‰ã€‚
- `Negate`ï¼šè®¡ç®—è¾“å…¥çš„æ•°å€¼è´Ÿå€¼ï¼Œé€å…ƒç´ æ“ä½œï¼ˆ1ä¸ªè¾“å…¥ï¼‰ã€‚
- `Transpose`ï¼šé¢ å€’ä¸¤ä¸ªè½´çš„é¡ºåºï¼Œé»˜è®¤ä¸ºæœ€åŽä¸¤è½´ï¼ˆ1ä¸ªè¾“å…¥ï¼Œ`axes` - å…ƒç»„ï¼‰ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äºŽåœ¨æœªæ¥çš„ä½œä¸šä¸­æˆ‘ä»¬å°†ä½¿ç”¨ `numpy`ä»¥å¤–çš„åŽç«¯ï¼Œè¿™é‡Œå°† `numpy`ä½œä¸º `array_api`å¯¼å…¥ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è°ƒç”¨ `array_api.add()`ç­‰å‡½æ•°ï¼Œå¦‚æžœæˆ‘ä»¬æƒ³ä½¿ç”¨å…¸åž‹çš„ `np.X()`è°ƒç”¨çš„è¯ã€‚

## PowerScalar

`PowerScalar`ç±»çš„ `compute`æ–¹æ³•è´Ÿè´£è¿›è¡Œå‰å‘è®¡ç®—ã€‚å…·ä½“åœ°ï¼Œå®ƒå°†è¾“å…¥çš„NDArray `a`æå‡åˆ° `self.scalar`æŒ‡å®šçš„æ•´æ•°å¹‚ã€‚åœ¨Pythonçš„ `numpy`åº“ä¸­ï¼Œè¿™ä¸€è®¡ç®—å¯ä»¥ç›´æŽ¥é€šè¿‡ `numpy.power`å‡½æ•°å®žçŽ°ã€‚ä¸‹é¢æ˜¯å¯¹åº”çš„å…¬å¼ï¼š

$$
a^{\text{scalar}}
$$

è¿™é‡Œï¼Œ`a`æ˜¯è¾“å…¥çš„å¤šç»´æ•°ç»„ï¼ˆNDArrayï¼‰ï¼Œè€Œ `scalar`æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºæˆ‘ä»¬è¦å°† `a`çš„æ¯ä¸ªå…ƒç´ éƒ½æå‡åˆ°çš„å¹‚æ¬¡ã€‚

åœ¨ä»£ç ä¸­ï¼Œè¿™ä¸ªè®¡ç®—è¢«å®žçŽ°å¦‚ä¸‹ï¼š

```python
def compute(self, a: NDArray) -> NDArray:
    return numpy.power(a, self.scalar)
```

## EWiseDiv

è¯¥ç±»ä»£è¡¨çš„æ“ä½œæ˜¯é€å…ƒç´ åœ°å°†ä¸¤ä¸ªèŠ‚ç‚¹è¿›è¡Œé™¤æ³•è¿ç®—ã€‚

`compute`æ–¹æ³•æ‰§è¡Œå‰å‘è®¡ç®—ï¼Œå®ƒæŽ¥æ”¶ä¸¤ä¸ªNDArrayå¯¹è±¡ `a`å’Œ `b`ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›žå®ƒä»¬é€å…ƒç´ ç›¸é™¤çš„ç»“æžœã€‚é€å…ƒç´ é™¤æ³•æ„å‘³ç€è¾“å‡ºæ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯è¾“å…¥æ•°ç»„å¯¹åº”å…ƒç´ çš„å•†ã€‚åœ¨ `numpy`ä¸­ï¼Œè¿™å¯ä»¥ç®€å•åœ°ä½¿ç”¨ `/`è¿ç®—ç¬¦å®Œæˆã€‚

ä¸‹é¢æ˜¯å¯¹åº”çš„Markdownæ ¼å¼å…¬å¼ï¼š

$$
a_i / b_i
$$

è¿™é‡Œ `a_i`å’Œ `b_i`æ˜¯æ•°ç»„ `a`å’Œ `b`ä¸­çš„å¯¹åº”å…ƒç´ ã€‚åœ¨ä»£ç ä¸­ï¼Œ`compute`æ–¹æ³•è¢«å®žçŽ°ä¸ºï¼š

```python
def compute(self, a, b):
    return a / b
```

## DivScalar

`DivScalar`ç±»å®žçŽ°äº†å°†ä¸€ä¸ªå¼ é‡ `a`çš„æ¯ä¸ªå…ƒç´ é™¤ä»¥ä¸€ä¸ªæ ‡é‡ `scalar`çš„æ“ä½œã€‚è¿™åœ¨æ•°å­¦ä¸Šè¡¨ç¤ºä¸ºå°†å¼ é‡ `a`çš„æ¯ä¸ªå…ƒç´  `a_i`é™¤ä»¥ `scalar`ï¼š

$$
\frac{a_i}{\text{scalar}}
$$

åœ¨ `compute`æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è¿™ä¸ªç®€å•çš„æ•°å­¦æ“ä½œã€‚è¿™åœ¨ä»£ç ä¸­è¢«å®žçŽ°ä¸ºï¼š

```python
def compute(self, a):
    return a / self.scalar
```

è¿™é‡Œï¼Œ`a`æ˜¯ä¸€ä¸ªNDArrayå¯¹è±¡ï¼Œ`self.scalar`æ˜¯åˆå§‹åŒ– `DivScalar`ç±»æ—¶ä¼ å…¥çš„æ ‡é‡å€¼ã€‚

## MatMul

`MatMul`ç±»çš„ `compute`æ–¹æ³•å®žçŽ°äº†ä¸¤ä¸ªçŸ©é˜µ `a`å’Œ `b`ä¹‹é—´çš„çŸ©é˜µä¹˜æ³•ã€‚çŸ©é˜µä¹˜æ³•æ˜¯çº¿æ€§ä»£æ•°ä¸­çš„ä¸€é¡¹åŸºç¡€æ“ä½œï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ç¬¬ä¸€ä¸ªçŸ©é˜µçš„è¡Œä¸Žç¬¬äºŒä¸ªçŸ©é˜µçš„åˆ—çš„ç‚¹ç§¯ã€‚åœ¨Pythonä¸­ï¼Œè¿™å¯ä»¥ä½¿ç”¨ `@`è¿ç®—ç¬¦æ¥ç®€åŒ–å®žçŽ°ï¼š

$$
\mathbf{C} = \mathbf{A} @ \mathbf{B}
$$

è¿™é‡Œï¼Œ$\mathbf{A}$å’Œ$\mathbf{B}$æ˜¯è¾“å…¥çŸ©é˜µï¼Œ$\mathbf{C}$æ˜¯ç»“æžœçŸ©é˜µã€‚åœ¨ä»£ç ä¸­ï¼Œ`compute`æ–¹æ³•è¢«å®žçŽ°ä¸ºï¼š

```python
def compute(self, a, b):
    return a @ b
```

## Summation

`Summation`ç±»å®žçŽ°äº†å¯¹å¼ é‡è¿›è¡Œæ±‚å’Œçš„æ“ä½œã€‚è¿™ä¸ªç±»çš„æ–¹æ³• `compute`è´Ÿè´£æ‰§è¡Œå®žé™…çš„æ±‚å’Œè®¡ç®—ã€‚

å¦‚æžœåœ¨åˆå§‹åŒ– `Summation`å¯¹è±¡æ—¶æ²¡æœ‰æŒ‡å®š `axes`å‚æ•°ï¼Œåˆ™é»˜è®¤å¯¹å¼ é‡ `a`çš„æ‰€æœ‰å…ƒç´ è¿›è¡Œå…¨å±€æ±‚å’Œã€‚å¦‚æžœæŒ‡å®šäº† `axes`ï¼Œåˆ™ä»…å¯¹æŒ‡å®šè½´ä¸Šçš„å…ƒç´ è¿›è¡Œæ±‚å’Œã€‚

`compute`æ–¹æ³•æ ¹æ®æ˜¯å¦æŒ‡å®šäº† `axes`æ¥å†³å®šæ±‚å’Œçš„æ–¹å¼ï¼š

- **å…¨å±€æ±‚å’Œ**ï¼šå¦‚æžœ `self.axes`ä¸º `None`ï¼Œåˆ™å¯¹å¼ é‡ `a`çš„æ‰€æœ‰å…ƒç´ è¿›è¡Œæ±‚å’Œã€‚
- **æŒ‡å®šè½´æ±‚å’Œ**ï¼šå¦‚æžœæŒ‡å®šäº† `self.axes`ï¼Œåˆ™åªåœ¨è¿™äº›ç‰¹å®šçš„è½´ä¸Šè¿›è¡Œæ±‚å’Œã€‚

ä»£ç å®žçŽ°å¦‚ä¸‹ï¼š

```python
def compute(self, a):
    if self.axes is None:
        return array_api.sum(a)
  
    return array_api.sum(a, axis=self.axes)
```

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œ`array_api.sum`æ˜¯æ‰§è¡Œæ±‚å’Œæ“ä½œçš„å‡½æ•°ï¼Œ`a`æ˜¯è¾“å…¥çš„å¼ é‡ï¼Œ`self.axes`æ˜¯ä¸€ä¸ªè½´çš„å…ƒç»„ï¼ŒæŒ‡ç¤ºäº†æ±‚å’Œæ“ä½œçš„ç»´åº¦ã€‚

## BroadcastTo

`BroadcastTo`ç±»è®¾è®¡ç”¨æ¥æ‰©å±•å¼ é‡çš„å½¢çŠ¶ï¼Œä½¿å…¶ç¬¦åˆæŸä¸ªæ–°çš„å½¢çŠ¶ã€‚è¿™ç§æ“ä½œåœ¨æ·±åº¦å­¦ä¹ ä¸­å¸¸è§ï¼Œå¦‚éœ€è¦å°†å°è§„æ¨¡æ•°æ®æ‰©å±•ä»¥ä¸Žå¤§è§„æ¨¡æ•°æ®è¿›è¡Œæ“ä½œæ—¶ã€‚

`compute`æ–¹æ³•è´Ÿè´£æ‰§è¡Œå®žé™…çš„å¹¿æ’­æ“ä½œï¼Œå…¶ä½¿ç”¨äº† `array_api.broadcast_to`å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°å°†è¾“å…¥çš„å¼ é‡ `a`æ‰©å±•åˆ° `self.shape`å®šä¹‰çš„æ–°å½¢çŠ¶ã€‚

- å¦‚æžœ `a`çš„å½¢çŠ¶å¯ä»¥åœ¨ä¸å¤åˆ¶æ•°æ®çš„å‰æä¸‹æ‰©å±•åˆ° `self.shape`ï¼Œåˆ™è¿›è¡Œå¹¿æ’­ã€‚
- å¦‚æžœ `a`çš„å½¢çŠ¶ä¸èƒ½å¹¿æ’­åˆ° `self.shape`ï¼Œé€šå¸¸ä¼šæŠ›å‡ºä¸€ä¸ªå¼‚å¸¸ã€‚

ä»£ç å®žçŽ°å¦‚ä¸‹ï¼š

```python
def compute(self, a):
    return array_api.broadcast_to(a, shape=self.shape)
```

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œ`a`æ˜¯è¾“å…¥çš„å¼ é‡ï¼Œ`self.shape`æ˜¯è¦å¹¿æ’­åˆ°çš„ç›®æ ‡å½¢çŠ¶ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œ`BroadcastTo`æ“ä½œå…è®¸ä¸åŒå½¢çŠ¶çš„å¼ é‡åœ¨æ•°å­¦è¿ç®—ä¸­å…¼å®¹ã€‚

## Reshape

`Reshape`ç±»ç”¨äºŽæ”¹å˜è¾“å…¥å¼ é‡ `a`çš„å½¢çŠ¶è€Œä¸æ”¹å˜å…¶æ•°æ®ã€‚è¿™åœ¨æ•°æ®é¢„å¤„ç†æˆ–ç½‘ç»œå±‚ä¹‹é—´ä¼ é€’æ•°æ®æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå½“ä½ éœ€è¦æ”¹å˜æ•°æ®çš„ç»´åº¦ä»¥é€‚åº”ç‰¹å®šæ“ä½œæ—¶ã€‚`compute`æ–¹æ³•æ‰§è¡Œå®žé™…çš„å½¢çŠ¶æ”¹å˜æ“ä½œã€‚å®ƒä¾èµ–äºŽ `array_api.reshape`å‡½æ•°ï¼Œè¯¥å‡½æ•°æŽ¥å—åŽŸå§‹å¼ é‡ `a`å’Œä¸€ä¸ªç›®æ ‡å½¢çŠ¶ `self.shape`ä½œä¸ºå‚æ•°ï¼Œå¹¶è¿”å›žä¸€ä¸ªæ–°å½¢çŠ¶çš„å¼ é‡ã€‚

ä»£ç å®žçŽ°å¦‚ä¸‹ï¼š

```python
def compute(self, a):
    return array_api.reshape(a, self.shape)
```

è¿™é‡Œï¼Œ`a`æ˜¯è¾“å…¥å¼ é‡ï¼Œ`self.shape`æ˜¯æˆ‘ä»¬æƒ³è¦ `a`é‡æ–°å¡‘å½¢æˆçš„æ–°å½¢çŠ¶ã€‚`Reshape`æ“ä½œç¡®ä¿äº†å¼ é‡ `a`çš„æ€»å…ƒç´ æ•°é‡ä¿æŒä¸å˜ï¼ŒåŒæ—¶å…è®¸æˆ‘ä»¬ä»¥æ–°çš„ç»´åº¦æŽ’åˆ—è¿™äº›å…ƒç´ ã€‚

## Negate

`Negate`ç±»å®žçŽ°äº†æ•°å€¼å–åæ“ä½œï¼Œå³å°†è¾“å…¥å¼ é‡ `a`ä¸­çš„æ‰€æœ‰å…ƒç´ çš„ç¬¦å·é¢ å€’ã€‚åœ¨æ•°å­¦å’Œç¼–ç¨‹ä¸­ï¼Œå–åæ˜¯ä¸€ä¸ªåŸºæœ¬æ“ä½œï¼Œé€šå¸¸ç”¨äºŽæ”¹å˜æ•°å€¼çš„æ­£è´Ÿã€‚`compute`æ–¹æ³•è´Ÿè´£æ‰§è¡Œå–åæ“ä½œã€‚å®ƒä½¿ç”¨äº† `array_api.negative`å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æŽ¥å—è¾“å…¥å¼ é‡ `a`ï¼Œå¹¶è¿”å›žä¸€ä¸ªæ–°å¼ é‡ï¼Œæ–°å¼ é‡çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ `a`ä¸­å¯¹åº”å…ƒç´ çš„è´Ÿå€¼ã€‚

ä»£ç å®žçŽ°å¦‚ä¸‹ï¼š

```python
def compute(self, a):
    return array_api.negative(a)
```

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œ`a`æ˜¯è¾“å…¥å¼ é‡ã€‚æ‰§è¡Œ `array_api.negative(a)`åŽï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œå…¶ä¸­åŒ…å«äº† `a`çš„æ•°å€¼å–ååŽçš„ç»“æžœã€‚è¿™ä¸ªæ“ä½œåœ¨æ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¸¸å¸¸æ¶‰åŠåˆ°æ¢¯åº¦çš„æ–¹å‘åè½¬ã€‚

## Transpose

è¯¥æ“ä½œå¯ä»¥äº¤æ¢å¤šç»´æ•°ç»„ä¸­ä»»æ„ä¸¤ä¸ªè½´çš„ä½ç½®ã€‚å®ƒçš„ç›®çš„æ˜¯è½¬ç½®è¾“å…¥çš„å¤šç»´æ•°ç»„ `a`ã€‚è½¬ç½®æ“ä½œé€šå¸¸æ„å‘³ç€åœ¨çŸ©é˜µï¼ˆäºŒç»´æ•°ç»„ï¼‰ä¸­äº¤æ¢è¡Œå’Œåˆ—ï¼Œä½†åœ¨å¤šç»´æ•°ç»„ä¸­ï¼Œè½¬ç½®å¯ä»¥æ›´ä¸€èˆ¬åŒ–ä¸ºäº¤æ¢ä»»æ„ä¸¤ä¸ªç»´åº¦ã€‚

ä»£ç é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ `self.axes`æä¾›ï¼Œ`self.axes`æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å…ƒç»„ï¼ŒæŒ‡å®šäº†éœ€è¦äº¤æ¢çš„è½´ã€‚å¦‚æžœæ²¡æœ‰æä¾› `self.axes`ï¼Œåˆ™é»˜è®¤äº¤æ¢æ•°ç»„çš„æœ€åŽä¸¤ä¸ªè½´ã€‚è¿™åœ¨å¤šç»´æ•°ç»„çš„çº¿æ€§ä»£æ•°æ“ä½œä¸­æ˜¯å¸¸è§çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰§è¡ŒçŸ©é˜µä¹˜æ³•æ—¶ã€‚

å¦‚æžœæä¾›äº† `self.axes`ï¼Œåˆ™ä»£ç æ ¹æ®æä¾›çš„è½´åˆ›å»ºä¸€ä¸ªæ–°çš„è½´é¡ºåºã€‚ç„¶åŽä½¿ç”¨ `array_api.transpose`å‡½æ•°æ‰§è¡Œè½¬ç½®ï¼Œè¿™ä¸ªå‡½æ•°æŽ¥æ”¶æ•°ç»„ `a`å’Œè½´é¡ºåº `axes_to_use`ä½œä¸ºå‚æ•°ã€‚

è½¬ç½®æ“ä½œå¯ä»¥ç”¨ä¸‹é¢çš„ä¼ªä»£ç æ¥æè¿°ï¼š

```markdown
# å¦‚æžœæœªæä¾›axesï¼Œä½¿ç”¨é»˜è®¤çš„æœ€åŽä¸¤ä¸ªè½´
if axes is None:
    transpose(a, axes=(-2, -1))
# å¦‚æžœæä¾›äº†axesï¼Œä½¿ç”¨æŒ‡å®šçš„è½´
else:
    transpose(a, axes=(axes[0], axes[1]))
```

åœ¨å®žé™…çš„Pythonä»£ç ä¸­ï¼Œè¿™ä¸ªé€»è¾‘æ˜¯è¿™æ ·å®žçŽ°çš„ï¼š

```python
def compute(self, a):
    if self.axes is None:
        axes_to_use = list(range(a.ndim))
        axes_to_use[-2], axes_to_use[-1] = axes_to_use[-1], axes_to_use[-2]
    else:
        axes_to_use = list(range(a.ndim))
        axes_to_use[self.axes[0]], axes_to_use[self.axes[1]] = axes_to_use[self.axes[1]], axes_to_use[self.axes[0]]
  
    return array_api.transpose(a, axes=axes_to_use)
```

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œ`a.ndim`è¡¨ç¤ºæ•°ç»„ `a`çš„ç»´åº¦æ•°ï¼Œ`list(range(a.ndim))`åˆ›å»ºäº†ä¸€ä¸ªä»Ž0åˆ° `a.ndim - 1`çš„æ•´æ•°åˆ—è¡¨ï¼Œä»£è¡¨äº†æ•°ç»„ `a`çš„åŽŸå§‹è½´é¡ºåºã€‚æŽ¥ä¸‹æ¥ï¼Œé€šè¿‡äº¤æ¢åˆ—è¡¨ä¸­çš„ä¸¤ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ–°çš„è½´é¡ºåºï¼Œç„¶åŽä¼ é€’ç»™ `array_api.transpose`å‡½æ•°æ¥å®žé™…æ‰§è¡Œè½¬ç½®æ“ä½œã€‚

## æ‰§è¡Œç»“æžœ

æ‰§è¡Œå…¨éƒ¨é€šè¿‡âœ…

![1699689035923](image/lab1_zh/f1.png)

# é—®é¢˜2ï¼šå®žçŽ°åå‘è®¡ç®—

## å¿…è¦çš„èƒŒæ™¯çŸ¥è¯†

ä¸»è¦æ˜¯åŸºäºŽé“¾å¼æ³•åˆ™ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹å‡ ä¸ªæ•°æ®ç»“æž„ï¼Œä¸»è¦æ˜¯ `Tensor` å’Œ `Value`ã€‚

`python/needle/autograd.py` ä¸­å®šä¹‰äº†ä¸€ä¸ªåä¸º `Value` çš„ç±»ï¼Œå®ƒä»£è¡¨äº†ä¸€ä¸ªè®¡ç®—å›¾ä¸­çš„èŠ‚ç‚¹ã€‚è¿™ä¸ªç±»çš„è®¾è®¡ç›®çš„æ˜¯ç”¨äºŽè‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿæˆ–è€…æ·±åº¦å­¦ä¹ æ¡†æž¶ä¸­ï¼Œä»¥è·Ÿè¸ªå’Œè®¡ç®—æ¢¯åº¦ã€‚ä»¥ä¸‹æ˜¯è¯¥ç±»çš„ä¸€äº›ä¸»è¦ç‰¹ç‚¹å’Œæ–¹æ³•ï¼š

- **å±žæ€§**:

  - `op`ï¼šä¸€ä¸ªå¯é€‰çš„ `Op` ç±»åž‹ï¼Œä»£è¡¨ä¸Žè¯¥å€¼ç›¸å…³çš„æ“ä½œã€‚
  - `inputs`ï¼šä¸€ä¸ª `Value` ç±»åž‹çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºè®¡ç®—å½“å‰å€¼æ‰€éœ€çš„è¾“å…¥å€¼åˆ—è¡¨ã€‚
  - `cached_data`ï¼šä¸€ä¸ªç¼“å­˜æ•°æ®ï¼Œç”¨äºŽå­˜å‚¨èŠ‚ç‚¹çš„å½“å‰å€¼ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚
  - `requires_grad`ï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºè¯¥å€¼æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚
- **æ–¹æ³•**:

  - `realize_cached_data`ï¼šè®¡ç®—æˆ–èŽ·å–ç¼“å­˜çš„æ•°æ®ã€‚å¦‚æžœ `cached_data` ä¸ä¸ºç©ºï¼Œåˆ™ç›´æŽ¥è¿”å›žç¼“å­˜çš„æ•°æ®ï¼Œå¦åˆ™ä½¿ç”¨ `op` çš„ `compute` æ–¹æ³•å’Œè¾“å…¥å€¼åˆ—è¡¨æ¥è®¡ç®—æ•°æ®ã€‚
  - `is_leaf`ï¼šåˆ¤æ–­è¯¥å€¼æ˜¯å¦æ˜¯å¶èŠ‚ç‚¹ï¼Œå³æ²¡æœ‰ `op` çš„å€¼ã€‚
  - `__del__`ï¼šæžæž„æ–¹æ³•ï¼Œç”¨äºŽæ›´æ–°å…¨å±€çš„ `TENSOR_COUNTER`ï¼Œåœ¨å€¼è¢«åˆ é™¤æ—¶å‡å°‘è®¡æ•°å™¨ã€‚
  - `_init`ï¼šåˆå§‹åŒ–æ–¹æ³•ï¼Œç”¨äºŽè®¾ç½® `op`ã€`inputs`ã€`num_outputs`ã€`cached_data` å’Œ `requires_grad`ã€‚å¦‚æžœ `requires_grad` æ²¡æœ‰æ˜Žç¡®æä¾›ï¼Œå®ƒä¼šæ ¹æ®è¾“å…¥å€¼çš„ `requires_grad` å±žæ€§ç¡®å®šã€‚
  - `make_const`ï¼šç±»æ–¹æ³•ï¼Œç”¨äºŽåˆ›å»ºä¸€ä¸ªå¸¸é‡å€¼ï¼Œå³æ²¡æœ‰æ“ä½œå’Œè¾“å…¥çš„å€¼ã€‚
  - `make_from_op`ï¼šç±»æ–¹æ³•ï¼Œç”¨äºŽæ ¹æ®æ“ä½œå’Œè¾“å…¥åˆ—è¡¨åˆ›å»ºä¸€ä¸ª `Value` å¯¹è±¡ã€‚å¦‚æžœä¸åœ¨æ‡’æƒ°æ¨¡å¼ä¸‹ï¼ˆ`LAZY_MODE` ä¸ºå‡ï¼‰ï¼Œå¹¶ä¸”è¯¥å€¼ä¸éœ€è¦æ¢¯åº¦ï¼Œå®ƒä¼šè¿”å›žä¸€ä¸ªåˆ†ç¦»çš„å€¼ã€‚å¦åˆ™ï¼Œä¼šç«‹å³è®¡ç®—ç¼“å­˜æ•°æ®ã€‚

è¿™ä¸ªç±»çš„è®¾è®¡å…è®¸æž„å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸­é—´æ“ä½œå’Œå€¼çš„è®¡ç®—å›¾ï¼Œä½¿å¾—å¯ä»¥è‡ªåŠ¨åœ°é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è®¡ç®—æ¢¯åº¦ã€‚è¿™æ˜¯çŽ°ä»£æ·±åº¦å­¦ä¹ åº“å¦‚ PyTorch å’Œ TensorFlow çš„åŸºç¡€ã€‚

 `Tensor` çš„ç±»ï¼Œå®ƒæ˜¯ `Value` ç±»çš„å­ç±»ï¼Œç”¨äºŽå®žçŽ°æ·±åº¦å­¦ä¹ æ¡†æž¶ä¸­çš„å¼ é‡æ“ä½œã€‚å¼ é‡æ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œå®ƒæ˜¯æ·±åº¦å­¦ä¹ ä¸­æ•°æ®è¡¨ç¤ºçš„æ ¸å¿ƒã€‚`Tensor` ç±»æä¾›äº†ç”¨äºŽåˆ›å»ºå’Œæ“ä½œè¿™äº›å¤šç»´æ•°ç»„çš„æŽ¥å£ï¼ŒåŒæ—¶æ”¯æŒè‡ªåŠ¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥ä¾¿ç”¨äºŽåå‘ä¼ æ’­ã€‚ä»¥ä¸‹æ˜¯è¯¥ç±»çš„ä¸€äº›ä¸»è¦ç‰¹ç‚¹å’Œæ–¹æ³•ï¼š

| ç±»åž‹                 | åç§°                  | æè¿°                                               |
| -------------------- | --------------------- | -------------------------------------------------- |
| **å±žæ€§**       | `grad`              | å­˜å‚¨æ¢¯åº¦çš„å¼ é‡ã€‚                                   |
| **æž„é€ å‡½æ•°**   | `__init__`          | åˆå§‹åŒ–å¼ é‡ï¼Œå¯æŒ‡å®šæ•°ç»„ã€è®¾å¤‡ã€æ•°æ®ç±»åž‹å’Œæ¢¯åº¦éœ€æ±‚ã€‚ |
| **é™æ€æ–¹æ³•**   | `_array_from_numpy` | å°† NumPy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ã€‚                          |
|                      | `make_from_op`      | æ ¹æ®æ“ä½œå’Œè¾“å…¥åˆ—è¡¨åˆ›å»ºå¼ é‡ã€‚                       |
|                      | `make_const`        | åˆ›å»ºä¸€ä¸ªå¸¸æ•°å¼ é‡ã€‚                                 |
| **å±žæ€§è®¿é—®å™¨** | `data`              | èŽ·å–æˆ–è®¾ç½®å¼ é‡æ•°æ®ã€‚                               |
|                      | `shape`             | è¿”å›žå¼ é‡å½¢çŠ¶ã€‚                                     |
|                      | `dtype`             | è¿”å›žæ•°æ®ç±»åž‹ã€‚                                     |
|                      | `device`            | è¿”å›žæ•°æ®æ‰€åœ¨çš„è®¾å¤‡ã€‚                               |
| **æ–¹æ³•**       | `detach`            | åˆ›å»ºä¸€ä¸ªå…±äº«æ•°æ®ä½†ä¸å‚ä¸Žæ¢¯åº¦è®¡ç®—çš„æ–°å¼ é‡ã€‚         |
|                      | `backward`          | å¯åŠ¨åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€‚                             |
|                      | `numpy`             | å°†å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ã€‚                          |
| **é­”æœ¯æ–¹æ³•**   | `__add__`ç­‰         | é‡è½½ç®—æœ¯æ“ä½œç¬¦ï¼Œæ”¯æŒç›´æŽ¥çš„å¼ é‡è®¡ç®—ã€‚               |
|                      | `__matmul__`        | é‡è½½çŸ©é˜µä¹˜æ³•æ“ä½œç¬¦ `@`ã€‚                         |
|                      | `__neg__`           | é‡è½½å–è´Ÿæ“ä½œ `-`ã€‚                               |
| **å…¶ä»–æ–¹æ³•**   | `sum`               | è®¡ç®—å’Œã€‚                                           |
|                      | `broadcast_to`      | å¹¿æ’­å¼ é‡åˆ°æ–°å½¢çŠ¶ã€‚                                 |
|                      | `reshape`           | æ”¹å˜å¼ é‡å½¢çŠ¶ã€‚                                     |
|                      | `transpose`         | è½¬ç½®å¼ é‡ã€‚                                         |

## ElementWise Addï¼šå°ä¾‹å­

### æ¢¯åº¦è®¡ç®—æ–¹æ³•

å¯¹äºŽ `EWiseAdd` æ“ä½œï¼Œç»™å®šçš„è¾“å‡ºæ¢¯åº¦ï¼ˆ`out_grad`ï¼‰ç›´æŽ¥å°±æ˜¯è¯¥æ“ä½œå¯¹äºŽæ¯ä¸ªè¾“å…¥å¼ é‡çš„æ¢¯åº¦ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æžœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ ‡é‡å‡½æ•° \( L \) æ˜¯æœ€ç»ˆçš„æŸå¤±å‡½æ•°ï¼Œ`a` å’Œ `b` æ˜¯ `EWiseAdd` æ“ä½œçš„è¾“å…¥ï¼Œé‚£ä¹ˆæ ¹æ®é“¾å¼æ³•åˆ™ï¼š

$$
\frac{\partial L}{\partial a} = \frac{\partial L}{\partial (a+b)} \cdot \frac{\partial (a+b)}{\partial a}
$$

å› ä¸º \( a + b \) ç›¸å¯¹äºŽ \( a \) çš„å¯¼æ•°æ˜¯ 1ï¼Œæˆ‘ä»¬å¯ä»¥ç®€åŒ–ä¸ºï¼š

$$
\frac{\partial L}{\partial a} = \frac{\partial L}{\partial (a+b)}
$$

åŒç†ï¼Œå¯¹ \( b \) ä¹Ÿæ˜¯å¦‚æ­¤ï¼š

$$
\frac{\partial L}{\partial b} = \frac{\partial L}{\partial (a+b)}
$$

å› æ­¤ï¼Œæ— è®ºè¾“å…¥å¼ é‡ `a` å’Œ `b` çš„å…·ä½“å†…å®¹æ˜¯ä»€ä¹ˆï¼Œ`EWiseAdd` æ“ä½œçš„æ¢¯åº¦éƒ½æ˜¯è¾“å‡ºæ¢¯åº¦ï¼ˆ`out_grad`ï¼‰è‡ªèº«ã€‚

### åœ¨ä»£ç ä¸­çš„å®žçŽ°

```python
class EWiseAdd(TensorOp):
    def compute(self, a: NDArray, b: NDArray):
        # ç›´æŽ¥è¿”å›žè¾“å…¥å¼ é‡aå’Œbçš„é€å…ƒç´ æ±‚å’Œç»“æžœ
        return a + b

    def gradient(self, out_grad: Tensor, node: Tensor):
        # è¿”å›žä¸Žè¾“å‡ºæ¢¯åº¦ç›¸åŒçš„æ¢¯åº¦ç»™æ¯ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹
        return out_grad, out_grad
```

è¿™æ®µä»£ç çš„ `gradient` æ–¹æ³•è¿”å›žäº†ä¸€ä¸ªå…ƒç»„ `(out_grad, out_grad)`ï¼Œå…¶ä¸­æ¯ä¸ª `out_grad` æ˜¯ä¼ æ’­åˆ°æ¯ä¸ªè¾“å…¥å¼ é‡ `a` å’Œ `b` çš„æ¢¯åº¦ã€‚åœ¨è‡ªåŠ¨å¾®åˆ†çš„èƒŒæ™¯ä¸‹ï¼Œè¿™æ„å‘³ç€æŸå¤±å‡½æ•°ç›¸å¯¹äºŽ `EWiseAdd` æ“ä½œçš„æ¯ä¸ªè¾“å…¥çš„æ¢¯åº¦ï¼Œéƒ½æ˜¯æŸå¤±å‡½æ•°ç›¸å¯¹äºŽè¯¥æ“ä½œè¾“å‡ºçš„æ¢¯åº¦ã€‚

## PowerScalar

`PowerScalar` ç®—å­æŽ¥æ”¶ä¸€ä¸ªæ•´æ•°æ ‡é‡ `scalar` å¹¶å°†è¾“å…¥å¼ é‡ `a` çš„æ¯ä¸ªå…ƒç´  \( a_i \) ä¹˜ä»¥è‡ªèº« `scalar` æ¬¡ã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

$$
a_i^{scalar}
$$

### æ¢¯åº¦è®¡ç®—

å½“æˆ‘ä»¬éœ€è¦å¯¹ `PowerScalar` ç®—å­è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬è¦è®¡ç®—å…³äºŽè¾“å…¥ `a` çš„æ¢¯åº¦ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œå¦‚æžœæœ‰ä¸€ä¸ªæ ‡é‡å‡½æ•° \( L \) æ˜¯æœ€ç»ˆçš„æŸå¤±å‡½æ•°ï¼Œé‚£ä¹ˆ `a` çš„æ¢¯åº¦æ˜¯ï¼š

$$
\frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial (a_i^{scalar})} \cdot \frac{\partial (a_i^{scalar})}{\partial a_i}
$$

ç»™å®š \( a_i^{scalar} \) ç›¸å¯¹äºŽ \( a_i \) çš„å¯¼æ•°æ˜¯ \( scalar \cdot a_i^{scalar - 1} \)ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å±•å¼€ä¸Šè¿°è¡¨è¾¾å¼ï¼š

$$
\frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial (a_i^{scalar})} \cdot scalar \cdot a_i^{scalar - 1}
$$

è¿™æ„å‘³ç€è¾“å…¥å¼ é‡ `a` ä¸­æ¯ä¸ªå…ƒç´  \( a_i \) çš„æ¢¯åº¦æ˜¯ç”±å¤–éƒ¨ä¼ é€’çš„æ¢¯åº¦ï¼ˆå³ \( \frac{\partial L}{\partial (a_i^{scalar})} \)ï¼‰ä¹˜ä»¥ `scalar` å†ä¹˜ä»¥ \( a_i \) çš„ `scalar - 1` æ¬¡å¹‚ã€‚

### ä»£ç å®žçŽ°

```python
class PowerScalar(TensorOp):
    """Op raise a tensor to an (integer) power."""
  
    def __init__(self, scalar: int):
        self.scalar = scalar

    def compute(self, a: NDArray) -> NDArray:
        # è®¡ç®—è¾“å…¥å¼ é‡açš„æ¯ä¸ªå…ƒç´ çš„scalaræ¬¡å¹‚
        return array_api.power(a, self.scalar)

    def gradient(self, out_grad: Tensor, node: Tensor):
        # è®¡ç®—æ¢¯åº¦
        return Tensor(out_grad * self.scalar * array_api.power(node, self.scalar - 1))
```

## EWiseDiv

`EWiseDiv` ç®—å­æŒ‰å…ƒç´ åœ°å°†ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆå¼ é‡ï¼‰ `a` é™¤ä»¥å¦ä¸€ä¸ªèŠ‚ç‚¹ `b`ã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

$$
c_i = \frac{a_i}{b_i}
$$

å…¶ä¸­ $c_i $ æ˜¯ç»“æžœå¼ é‡ä¸­çš„ç¬¬ \( i \) ä¸ªå…ƒç´ ã€‚

### æ¢¯åº¦è®¡ç®—

ä¸ºäº†åœ¨ç¥žç»ç½‘ç»œä¸­åå‘ä¼ æ’­è¯¯å·®ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®— `EWiseDiv` æ“ä½œç›¸å¯¹äºŽå…¶ä¸¤ä¸ªè¾“å…¥çš„æ¢¯åº¦ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œå¦‚æžœæœ‰ä¸€ä¸ªæ ‡é‡å‡½æ•° \( L \) æ˜¯æœ€ç»ˆçš„æŸå¤±å‡½æ•°ï¼Œé‚£ä¹ˆå…³äºŽ `a` å’Œ `b` çš„æ¢¯åº¦åˆ†åˆ«æ˜¯ï¼š

å¯¹äºŽ \( a \)ï¼š

$$
\frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial c_i} \cdot \frac{\partial c_i}{\partial a_i}
$$

å› ä¸º $ \frac{\partial c_i}{\partial a_i} = \frac{1}{b_i} $ï¼Œæ‰€ä»¥ï¼š

$$
\frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial c_i} \cdot \frac{1}{b_i}
$$

å¯¹äºŽ \( b \)ï¼š

$$
\frac{\partial L}{\partial b_i} = \frac{\partial L}{\partial c_i} \cdot \frac{\partial c_i}{\partial b_i}
$$

ç”±äºŽ $ \frac{\partial c_i}{\partial b_i} = -\frac{a_i}{b_i^2} $ï¼Œæˆ‘ä»¬æœ‰ï¼š

$$
\frac{\partial L}{\partial b_i} = -\frac{\partial L}{\partial c_i} \cdot \frac{a_i}{b_i^2}
$$

### ä»£ç å®žçŽ°

```python
class EWiseDiv(TensorOp):
    """Op to element-wise divide two nodes."""
  
    def compute(self, a, b):
        # è¿”å›žè¾“å…¥å¼ é‡aå’Œbçš„é€å…ƒç´ å•†
        return a / b

    def gradient(self, out_grad, node):
        a, b = node.inputs 
        # å¯¹äºŽaçš„æ¢¯åº¦
        grad_a = out_grad / b
        # å¯¹äºŽbçš„æ¢¯åº¦
        grad_b = -out_grad * a / array_api.power(b, 2)
        return Tensor(grad_a), Tensor(grad_b)
```

## DivScalar

`DivScalar` ç®—å­å°†è¾“å…¥å¼ é‡ `a` çš„æ¯ä¸ªå…ƒç´  $ a_i $ é™¤ä»¥ä¸€ä¸ªæ ‡é‡å€¼ `scalar`ã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

$$
c_i = \frac{a_i}{scalar}
$$

å…¶ä¸­ $ c_i $ æ˜¯ç»“æžœå¼ é‡ä¸­çš„ç¬¬ \( i \) ä¸ªå…ƒç´ ã€‚

### æ¢¯åº¦è®¡ç®—

å¯¹äºŽ `DivScalar` ç®—å­ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å®ƒç›¸å¯¹äºŽè¾“å…¥å¼ é‡ `a` çš„æ¢¯åº¦ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œå¦‚æžœæœ‰ä¸€ä¸ªæ ‡é‡å‡½æ•° \( L \) ä»£è¡¨æœ€ç»ˆçš„æŸå¤±å‡½æ•°ï¼Œé‚£ä¹ˆå¯¹äºŽè¾“å…¥å¼ é‡ `a` ä¸­çš„æ¯ä¸ªå…ƒç´  $a_i $ çš„æ¢¯åº¦æ˜¯ï¼š

$$
\frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial c_i} \cdot \frac{\partial c_i}{\partial a_i}
$$

ç”±äºŽ `scalar` æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå› æ­¤ $ \frac{\partial c_i}{\partial a_i} $ çš„å€¼ä¸º $ \frac{1}{scalar} $ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

$$
\frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial c_i} \cdot \frac{1}{scalar}
$$

### ä»£ç å®žçŽ°

```python
class DivScalar(TensorOp):
    def __init__(self, scalar):
        self.scalar = scalar

    def compute(self, a):
        # è¿”å›žè¾“å…¥å¼ é‡aä¸Žæ ‡é‡scalarçš„å•†
        return a / self.scalar

    def gradient(self, out_grad: Tensor, node: Tensor):
        # è¿”å›žè¾“å…¥å¼ é‡açš„æ¢¯åº¦
        return out_grad * (1 / self.scalar)
```

## MatMul

åœ¨ç¥žç»ç½‘ç»œä¸­ï¼ŒçŸ©é˜µä¹˜æ³•æ˜¯ä¸€ä¸ªåŸºç¡€ä¸”å¸¸ç”¨çš„è¿ç®—ï¼Œç”¨äºŽå±‚ä¹‹é—´çš„çº¿æ€§å˜æ¢ã€‚`MatMul` ç®—å­å®žçŽ°äº†ä¸¤ä¸ªå¼ é‡çš„çŸ©é˜µä¹˜æ³•ã€‚å¯¹äºŽæ¢¯åº¦çš„è®¡ç®—ï¼Œç‰¹åˆ«æ˜¯å½“æ¶‰åŠåˆ°æ‰¹å¤„ç†å’Œä¸åŒç»´åº¦çš„å¼ é‡æ—¶ï¼Œéœ€è¦ç‰¹åˆ«å¤„ç†ä»¥ä¿è¯æ¢¯åº¦çš„å½¢çŠ¶ä¸ŽåŽŸå¼ é‡åŒ¹é…ã€‚

`MatMul` ç®—å­å®žçŽ°äº†ä¸¤ä¸ªå¼ é‡ `a` å’Œ `b` çš„çŸ©é˜µä¹˜æ³•ã€‚å¦‚æžœå¼ é‡ `a` çš„å½¢çŠ¶æ˜¯ \( m \times n \) è€Œå¼ é‡ `b` çš„å½¢çŠ¶æ˜¯ \( n \times p \)ï¼Œé‚£ä¹ˆç»“æžœå¼ é‡çš„å½¢çŠ¶å°†æ˜¯ \( m \times p \)ã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

$$
C = AB
$$

å…¶ä¸­ \( C \) æ˜¯ç»“æžœçŸ©é˜µï¼Œ\( A \) æ˜¯å·¦çŸ©é˜µï¼Œ\( B \) æ˜¯å³çŸ©é˜µã€‚

### æ¢¯åº¦è®¡ç®—

å‡è®¾æœ‰ä¸€ä¸ªæ ‡é‡å‡½æ•° \( L \) è¡¨ç¤ºæŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®— `MatMul` æ“ä½œç›¸å¯¹äºŽå…¶è¾“å…¥ `a` å’Œ `b` çš„æ¢¯åº¦ã€‚åˆ©ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬æœ‰ï¼š

å¯¹äºŽ \( A \) çš„æ¢¯åº¦ï¼š

$$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} \cdot B^T
$$

å¯¹äºŽ \( B \) çš„æ¢¯åº¦ï¼š

$$
\frac{\partial L}{\partial B} = A^T \cdot \frac{\partial L}{\partial C}
$$

å…¶ä¸­ \( B^T \) å’Œ \( A^T \) åˆ†åˆ«è¡¨ç¤º \( B \) å’Œ \( A \) çš„è½¬ç½®ã€‚

### ä»£ç å®žçŽ°

```python
class MatMul(TensorOp):
    def compute(self, a, b):
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
        return array_api.matmul(a, b)

    @staticmethod
    def match_shape(grad, original_shape):
        # è°ƒæ•´æ¢¯åº¦å½¢çŠ¶ä»¥åŒ¹é…åŽŸå§‹å¼ é‡å½¢çŠ¶
        # å¦‚æžœæ¢¯åº¦çš„ç»´æ•°æ¯”åŽŸå§‹å½¢çŠ¶å¤šï¼Œæ²¿ç€é¢å¤–çš„è½´æ±‚å’Œ
        # å¦‚æžœæ¢¯åº¦çš„ç»´æ•°æ¯”åŽŸå§‹å½¢çŠ¶å°‘ï¼Œå‰é¢æ·»åŠ ç»´åº¦
        while grad.ndim > len(original_shape):
            grad = grad.sum(axis=0)
        while grad.ndim < len(original_shape):
            grad = np.expand_dims(grad, axis=0)
        return grad

    def gradient(self, out_grad, node):
        a, b = node.inputs
        # è®¡ç®—açš„æ¢¯åº¦
        grad_a = self.compute(out_grad.cached_data, transpose(b).cached_data)
        grad_a = self.match_shape(grad_a, a.shape)
        # è®¡ç®—bçš„æ¢¯åº¦
        grad_b = self.compute(transpose(a).cached_data, out_grad.cached_data)
        grad_b = self.match_shape(grad_b, b.shape)
        return Tensor(grad_a), Tensor(grad_b)
```

åœ¨ `gradient` æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ ¹æ®é“¾å¼æ³•åˆ™è®¡ç®—äº†å¼ é‡ `a` å’Œ `b` çš„æ¢¯åº¦ã€‚ç„¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ `match_shape` é™æ€æ–¹æ³•æ¥è°ƒæ•´æ¢¯åº¦çš„å½¢çŠ¶ï¼Œä»¥ç¡®ä¿æ¢¯åº¦å¼ é‡çš„å½¢çŠ¶ä¸ŽåŽŸå§‹è¾“å…¥å¼ é‡ç›¸åŒ¹é…ã€‚

## Summation

`Summation` ç®—å­å¯¹è¾“å…¥å¼ é‡ `a` è¿›è¡Œæ±‚å’Œæ“ä½œï¼Œå¯ä»¥æŒ‡å®šåœ¨æŸäº›è½´ä¸Šè¿›è¡Œï¼Œä¹Ÿå¯ä»¥å¯¹æ•´ä¸ªå¼ é‡è¿›è¡Œæ±‚å’Œã€‚æ•°å­¦è¡¨è¾¾å¼å¦‚ä¸‹ï¼š

- åœ¨æ‰€æœ‰è½´ä¸Šæ±‚å’Œï¼ˆå¦‚æžœ `axes` ä¸º `None`ï¼‰ï¼š

$$
S = \sum_i a_i
$$

- åœ¨ç‰¹å®šè½´ä¸Šæ±‚å’Œï¼ˆå¦‚æžœ `axes` éž `None`ï¼‰ï¼š

$$
S_{j_1, \ldots, j_m} = \sum_{k} a_{j_1, \ldots, j_{m-1}, k, j_{m+1}, \ldots}
$$

å…¶ä¸­ \( j_1, \ldots, j_m \) æ˜¯éžæ±‚å’Œè½´ä¸Šçš„ç´¢å¼•ï¼Œ\( k \) æ˜¯æ±‚å’Œè½´ä¸Šçš„ç´¢å¼•ã€‚

### æ¢¯åº¦è®¡ç®—

æ ¹æ®å¾®åˆ†çš„é“¾å¼æ³•åˆ™ï¼Œæ±‚å’Œæ“ä½œçš„æ¢¯åº¦è®¡ç®—å¦‚ä¸‹ï¼š

- å¦‚æžœæ±‚å’Œæ˜¯åœ¨æ‰€æœ‰è½´ä¸Šè¿›è¡Œçš„ï¼Œæ¢¯åº¦å°†æ˜¯ä¸€ä¸ªä¸ŽåŽŸå§‹è¾“å…¥å¼ é‡ `a` å½¢çŠ¶ç›¸åŒçš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯æ±‚å’Œç»“æžœçš„æ¢¯åº¦ `out_grad`ã€‚
- å¦‚æžœæ±‚å’Œæ˜¯åœ¨ç‰¹å®šè½´ä¸Šè¿›è¡Œçš„ï¼Œæ¢¯åº¦å°†æ˜¯ä¸€ä¸ªå¹¿æ’­ï¼ˆbroadcastï¼‰åŽçš„å¼ é‡ï¼Œå…¶åœ¨æ±‚å’Œè½´ä¸Šçš„å¤§å°ä¸º1ï¼Œåœ¨å…¶ä»–è½´ä¸Šä¸Žè¾“å…¥å¼ é‡ `a` ä¿æŒä¸€è‡´ã€‚

æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

$$
\frac{\partial L}{\partial a_i} =
\begin{cases}
out\_grad, & \text{if sum over all axes} \\
broadcast(out\_grad), & \text{if sum over certain axes}
\end{cases}
$$

å…¶ä¸­ \( L \) æ˜¯æŸå¤±å‡½æ•°ã€‚

### ä»£ç å®žçŽ°

```python
class Summation(TensorOp):
    def __init__(self, axes: Optional[tuple] = None):
        self.axes = axes

    def compute(self, a):
        # æ ¹æ®æŒ‡å®šçš„è½´è¿›è¡Œæ±‚å’Œ
        return array_api.sum(a, axis=self.axes) if self.axes is not None else array_api.sum(a)

    def gradient(self, out_grad, node):
        input_value = node.inputs[0]  # è¾“å…¥å¼ é‡a

        # æ ¹æ®æ±‚å’Œçš„è½´è°ƒæ•´æ¢¯åº¦çš„å½¢çŠ¶ä»¥åŒ¹é…è¾“å…¥å¼ é‡açš„å½¢çŠ¶
        if self.axes is not None:
            # åˆ›å»ºä¸€ä¸ªæ–°çš„å½¢çŠ¶ï¼Œæ±‚å’Œè½´çš„å¤§å°ä¸º1ï¼Œå…¶ä»–è½´ä¿æŒä¸å˜
            output_shape = [size if axis not in self.axes else 1 for axis, size in enumerate(input_value.shape)]
            # å¹¿æ’­æ¢¯åº¦åˆ°æ–°çš„å½¢çŠ¶
            grad = array_api.broadcast_to(Reshape(output_shape).compute(out_grad.cached_data), input_value.shape)
        else:
            # å¦‚æžœåœ¨æ‰€æœ‰è½´ä¸Šæ±‚å’Œï¼Œæ¢¯åº¦æ˜¯ä¸€ä¸ªå¡«å……äº†out_gradå€¼çš„ä¸Žè¾“å…¥å¼ é‡aå½¢çŠ¶ç›¸åŒçš„å¼ é‡
            grad = array_api.full_like(input_value, out_grad.cached_data)

        return Tensor(grad)
```

## BroadcastTo

`BroadcastTo` ç®—å­å°†è¾“å…¥å¼ é‡ `a` æ‰©å±•åˆ°ç»™å®šçš„å½¢çŠ¶ `shape`ã€‚åœ¨æ•°å­¦ä¸Šï¼Œå¯¹äºŽå¼ é‡ `a` çš„æ¯ä¸ªå…ƒç´  \( a_{i_1, \ldots, i_n} \)ï¼Œå¦‚æžœåœ¨ç»™å®šç»´åº¦ä¸Šè¿›è¡Œäº†å¹¿æ’­ï¼Œåˆ™è¯¥ç»´åº¦ä¸Šæ‰€æœ‰çš„å…ƒç´ å€¼å°†ä¸Ž \( a_{i_1, \ldots, i_n} \) ç›¸åŒã€‚

### æ¢¯åº¦è®¡ç®—

å½“è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»å°†å¹¿æ’­åŽçš„æ¢¯åº¦ `out_grad` å‡å°‘åˆ°è¾“å…¥å¼ é‡ `a` çš„åŽŸå§‹å½¢çŠ¶ã€‚ç”±äºŽåœ¨å¹¿æ’­è¿‡ç¨‹ä¸­ï¼ŒåŽŸå§‹å¼ é‡çš„å•ä¸€å…ƒç´ è¢«å¤åˆ¶åˆ°äº†å¤šä¸ªä½ç½®ï¼Œå› æ­¤åœ¨è®¡ç®—æ¢¯åº¦æ—¶éœ€è¦æ²¿è¢«å¹¿æ’­çš„ç»´åº¦æ±‚å’Œã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æžœæŸä¸ªç»´åº¦è¢«å¹¿æ’­ï¼ˆå³åŽŸå§‹å½¢çŠ¶ä¸­è¯¥ç»´åº¦å¤§å°ä¸º1ï¼‰ï¼Œåˆ™åœ¨è¯¥ç»´åº¦ä¸Šçš„æ¢¯åº¦å°†è¢«æ±‚å’Œã€‚

æ•°å­¦è¡¨è¾¾å¼å¦‚ä¸‹ï¼š

$$
\frac{\partial L}{\partial a_{i_1, \ldots, i_n}} = \sum_{j_1, \ldots, j_m} \frac{\partial L}{\partial c_{j_1, \ldots, j_m}}
$$

å…¶ä¸­ \( \frac{\partial L}{\partial c_{j_1, \ldots, j_m}} \) è¡¨ç¤ºå¹¿æ’­åŽå¼ é‡çš„æ¢¯åº¦ï¼Œ\( \{j_1, \ldots, j_m\} \) æ˜¯å¹¿æ’­æ“ä½œä¸­å¤åˆ¶ \( a_{i_1, \ldots, i_n} \) çš„ç´¢å¼•é›†åˆã€‚

### ä»£ç å®žçŽ°

```python
class BroadcastTo(TensorOp):
    def __init__(self, shape):
        self.shape = shape

    def compute(self, a):
        # æ‰§è¡Œå¹¿æ’­æ“ä½œ
        return array_api.broadcast_to(a, shape=self.shape)

    def gradient(self, out_grad, node):
        input_shape = node.inputs[0].shape  # è¾“å…¥å¼ é‡çš„å½¢çŠ¶
        output_shape = self.shape  # å¹¿æ’­åŽçš„å½¢çŠ¶

        # åˆå§‹åŒ–æ¢¯åº¦å½¢çŠ¶å…¨1ï¼Œç”¨äºŽç¡®å®šæ±‚å’Œç»´åº¦
        grad_shape = [1] * len(output_shape)

        # å¯¹äºŽæ¯ä¸ªç»´åº¦ï¼Œå¦‚æžœæœªå¹¿æ’­ï¼ˆå½¢çŠ¶å¤§å°ä¸ä¸º1ï¼‰ï¼Œæ¢¯åº¦å½¢çŠ¶ä¸Žè¾“å…¥ç›¸åŒ
        for i, (input_size, output_size) in enumerate(zip(input_shape, output_shape)):
            if input_size != 1:
                grad_shape[i] = input_size

        # æ²¿è¢«å¹¿æ’­çš„ç»´åº¦æ±‚å’Œä»¥åŒ¹é…è¾“å…¥å½¢çŠ¶
        grad = array_api.sum(out_grad.cached_data, axis=tuple(i for i, size in enumerate(grad_shape) if size == 1))
  
        # è°ƒæ•´æ¢¯åº¦å½¢çŠ¶
        grad = array_api.reshape(grad, input_shape)

        return Tensor(grad)
```

## Reshape

`Reshape` ç®—å­å…è®¸æˆ‘ä»¬æ”¹å˜å¼ é‡çš„å½¢çŠ¶è€Œä¸æ”¹å˜å…¶åŒ…å«çš„æ•°æ®å’Œæ•°æ®é¡ºåºã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå½¢çŠ¶ä¸º \( m \times n \) çš„å¼ é‡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶é‡æ–°å¡‘å½¢ä¸ºå½¢çŠ¶ä¸º \( p \times q \) çš„æ–°å¼ é‡ï¼Œåªè¦ \( m \times n = p \times q \)ã€‚

### æ¢¯åº¦è®¡ç®—

`Reshape` æ“ä½œæœ¬èº«ä¸ä¼šæ”¹å˜å¼ é‡ä¸­çš„æ•°æ®ï¼Œåªæ˜¯æ”¹å˜æ•°æ®çš„æŽ’åˆ—æ–¹å¼ã€‚å› æ­¤ï¼Œåœ¨åå‘ä¼ æ’­ä¸­ï¼Œæ¢¯åº¦ä¼ æ’­åˆ° `Reshape` æ“ä½œä»…éœ€è¦å°†æ¢¯åº¦é‡æ–°å¡‘å½¢å›žæ“ä½œå‰çš„å¼ é‡å½¢çŠ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæ¢¯åº¦çš„å½¢çŠ¶å˜æ¢éœ€è¦éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š

$$
\text{å¦‚æžœ} \quad a \in \mathbb{R}^{m \times n} \quad \text{è¢«é‡å¡‘ä¸º} \quad b \in \mathbb{R}^{p \times q}
$$

$$
\text{é‚£ä¹ˆ} \quad \frac{\partial L}{\partial a} \quad \text{å¯ä»¥é€šè¿‡å°†} \quad \frac{\partial L}{\partial b} \quad \text{é‡å¡‘ä¸º} \quad m \times n \quad \text{çš„å½¢çŠ¶æ¥å¾—åˆ°}
$$

å…¶ä¸­ \( L \) æ˜¯æŸå¤±å‡½æ•°ã€‚

### ä»£ç å®žçŽ°

```python
class Reshape(TensorOp):
    def __init__(self, shape):
        self.shape = shape

    def compute(self, a):
        # æ‰§è¡Œé‡å¡‘æ“ä½œ
        return array_api.reshape(a, self.shape) if self.shape is not None else a

    def gradient(self, out_grad, node):
        # èŽ·å–è¾“å…¥å¼ é‡çš„å½¢çŠ¶
        input_shape = node.inputs[0].cached_data.shape

        # å°†è¾“å‡ºæ¢¯åº¦é‡å¡‘å›žè¾“å…¥å¼ é‡çš„å½¢çŠ¶
        grad = array_api.reshape(out_grad.cached_data, input_shape)
        return Tensor(grad)
```

## Negate

`Negate` ç®—å­å°†è¾“å…¥å¼ é‡ `a` ä¸­çš„æ¯ä¸ªå…ƒç´ å–åã€‚å¦‚æžœè¾“å…¥å¼ é‡ `a` ä¸­çš„å…ƒç´ ä¸º \( a_i \)ï¼Œé‚£ä¹ˆç»è¿‡ `Negate` ç®—å­å¤„ç†åŽçš„å…ƒç´ ä¸º \( -a_i \)ã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

$$
c_i = -a_i
$$

å…¶ä¸­ \( c_i \) æ˜¯ç»“æžœå¼ é‡ä¸­çš„ç¬¬ \( i \) ä¸ªå…ƒç´ ã€‚

### æ¢¯åº¦è®¡ç®—

å¯¹äºŽ `Negate` ç®—å­ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å®ƒç›¸å¯¹äºŽè¾“å…¥å¼ é‡ `a` çš„æ¢¯åº¦ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œå¦‚æžœæœ‰ä¸€ä¸ªæ ‡é‡å‡½æ•° \( L \) ä»£è¡¨æœ€ç»ˆçš„æŸå¤±å‡½æ•°ï¼Œé‚£ä¹ˆå¯¹äºŽè¾“å…¥å¼ é‡ `a` çš„æ¢¯åº¦æ˜¯ï¼š

$$
\frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial c_i} \cdot \frac{\partial c_i}{\partial a_i}
$$

ç”±äºŽ \( c_i \) ç›¸å¯¹äºŽ \( a_i \) çš„å¯¼æ•°æ˜¯ -1ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

$$
\frac{\partial L}{\partial a_i} = -\frac{\partial L}{\partial c_i}
$$

è¿™æ„å‘³ç€ï¼Œ`Negate` ç®—å­çš„æ¢¯åº¦ä»…ä»…æ˜¯è¾“å‡ºæ¢¯åº¦ `out_grad` çš„ç›¸åæ•°ã€‚

### ä»£ç å®žçŽ°

```python
class Negate(TensorOp):
    def compute(self, a):
        # è®¡ç®—è¾“å…¥å¼ é‡açš„é€å…ƒç´ è´Ÿå€¼
        return array_api.negative(a)
  
    def gradient(self, out_grad, node):
        # è¿”å›žè¾“å‡ºæ¢¯åº¦çš„ç›¸åæ•°
        return -out_grad
```

## Transpose

`Transpose` ç®—å­èƒ½å¤Ÿæ ¹æ®æŒ‡å®šçš„ `axes` å‚æ•°æ¥äº¤æ¢å¼ é‡çš„ç»´åº¦ã€‚å¦‚æžœæ²¡æœ‰æŒ‡å®š `axes`ï¼Œé»˜è®¤æƒ…å†µä¸‹å°†äº¤æ¢æœ€åŽä¸¤ä¸ªç»´åº¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœè¾“å…¥æ˜¯ä¸€ä¸ªçŸ©é˜µï¼ˆ2ç»´å¼ é‡ï¼‰ï¼Œé‚£ä¹ˆè½¬ç½®æ“ä½œå°†å…¶è¡Œå’Œåˆ—äº¤æ¢ã€‚

### æ¢¯åº¦è®¡ç®—

å¯¹äºŽè½¬ç½®æ“ä½œï¼Œæ¢¯åº¦çš„è®¡ç®—ç›¸å¯¹ç›´è§‚ã€‚æ¢¯åº¦å¿…é¡»æ ¹æ®è½¬ç½®æ“ä½œé€†è½¬å›žåŽ»ï¼Œè¿™æ ·æ¯ä¸ªå…ƒç´ çš„æ¢¯åº¦æ‰ä¼šå›žåˆ°åŽŸå§‹çš„ä½ç½®ã€‚å¦‚æžœæˆ‘ä»¬å°†ä¸€ä¸ªå½¢çŠ¶ä¸º \( m \times n \) çš„å¼ é‡è¿›è¡Œè½¬ç½®ï¼Œå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸º \( n \times m \) çš„æ–°å¼ é‡ï¼Œé‚£ä¹ˆæ¢¯åº¦ä¹Ÿä¼šä»Ž \( n \times m \) è½¬ç½®å›ž \( m \times n \)ã€‚

æ•°å­¦ä¸Šï¼Œå¦‚æžœè½¬ç½®æ“ä½œç”±ä¸€ä¸ªç½®æ¢çŸ©é˜µ \( P \) è¡¨ç¤ºï¼Œä½¿å¾— \( B = PA \)ï¼Œé‚£ä¹ˆ \( A \) ç›¸å¯¹äºŽ \( B \) çš„æ¢¯åº¦ $ \frac{\partial L}{\partial A} $å¯ä»¥é€šè¿‡ \( P^T \frac{\partial L}{\partial B} \) å¾—åˆ°ï¼Œå…¶ä¸­ \( L \) æ˜¯æŸå¤±å‡½æ•°ã€‚

### ä»£ç å®žçŽ°

```python
class Transpose(TensorOp):
    def __init__(self, axes: Optional[tuple] = None):
        self.axes = axes

    def compute(self, a):
        # ç¡®å®šè½¬ç½®çš„è½´é¡ºåº
        axes_to_use = self.axes or list(range(a.ndim))[::-1]
        return array_api.transpose(a, axes=axes_to_use)

    def gradient(self, out_grad, node):
        # è®¡ç®—è½¬ç½®çš„æ¢¯åº¦ï¼Œå³é€†è½¬ç½®æ“ä½œ
        transposed_out_grad = self.compute(out_grad.cached_data)
        return Tensor(transposed_out_grad)
```

## ç»“æžœ

![1699774244417](image/lab1_zh/f2.png)

# æ€»ç»“

è¿™ç¯‡åšå®¢å®Œæˆäº†ä¸€äº›ç®—å­çš„å‰å‘è®¡ç®—å’Œæ¢¯åº¦ä¼ æ’­çš„é€»è¾‘ï¼Œå½“ç„¶lab1è¿˜æœ‰ä¸€äº›å…¶ä»–çš„ç»ƒä¹ ï¼Œè®©æˆ‘ä»¬ä¸‹ç¯‡åšå®¢å†è§ðŸ‘‹
