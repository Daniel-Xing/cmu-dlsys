# æ·±åº¦å­¦ä¹ ç³»ç»Ÿä»å…¥é—¨åˆ°æ”¾å¼ƒ - CMU-DL System Lab0

[CMU-DL System](https://dlsyscourse.org/assignments)æ˜¯é™ˆå¤©å¥‡æ•™æˆçš„è¯¾ç¨‹ï¼Œæ—¨åœ¨æ­ç¤ºMLç³»ç»Ÿçš„æ ¸å¿ƒåŸç†ã€é…åˆä¸€äº›é«˜è´¨é‡çš„assignmentï¼Œæœ¬æ–‡æ˜¯è¯¥ç³»åˆ—çš„ç¬¬ä¸€ç« èŠ‚ã€‚

ç«‹ä¸€ä¸ªflagï¼Œä¼šæŒç»­æ›´æ–°åç»­å®ç°ã€‚

ç›¸å…³ä»£ç æ”¾åœ¨äº†github: [danielxing-cmu-system](https://github.com/Daniel-Xing/cmu-dlsys)

# Lab0: è¯¾å‰è‡ªæµ‹

Lab0 ç»™å‡ºäº†ä¸€äº›æµ‹è¯•é¢˜ç›®ï¼Œè¿™äº›æµ‹è¯•é¢˜ç›®æœ‰åŠ©äºå­¦ä¹ è€…è¡¡é‡è‡ªå·±çš„èƒŒæ™¯æ˜¯å¦èƒ½å¤Ÿæ»¡è¶³å®Œæˆè¿™äº›è¯¾ç¨‹çš„å‰ç½®æ¡ä»¶ã€‚

é¦–å…ˆéœ€è¦å»Githubä¸Šå…‹éš†Lab0ï¼Œåœ°å€æ˜¯ï¼š

å…‹éš†å®Œæˆåæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸»è¦çš„ç›®å½•ç»“æ„ä¸ºï¼š

```Bash
data/
	train-images-idx3-ubyte.gz
	train-labels-idx1-ubyte.gz
	t10k-images-idx3-ubyte.gz
	t10k-labels-idx1-ubyte.gz
src/
	simple_ml.py
	simple_ml_ext.cpp
tests/
	test_simple_ml.py
Makefile
```

## é—®é¢˜1: å®ç°addæ–¹æ³•

é¦–å…ˆéœ€è¦åœ¨ `simple_ml.py` ä¸­å®ç°ä¸€ä¸ªç®€å•çš„addæ–¹æ³•ï¼Œå·²ç»™å‡ºçš„éª¨æ¶å¦‚ä¸‹ï¼š

```python
def add(x, y):
    """ A trivial 'add' function you should implement to get used to the 
    autograder and submission system.  The solution to this problem is in
    the homework notebook.

    Args:
        x (Python number or numpy array)
        y (Python number or numpy array)

    Return:
        Sum of x + y
    """
    ### YOUR CODE HERE
    pass
    ### END YOUR CODE
```

å®ç°éå¸¸ç®€å•ï¼Œç›´æ¥ç›¸åŠ å°±å¯ä»¥äº†

```Python
def add(x, y):
	...

	### BEGIN YOUR CODE

	return x + y

	### END YOUR CODE
```

## é—®é¢˜2:  åŠ è½½Ministæ•°æ®é›†

æ ¹æ®æç¤ºè¯»å–æ•°æ®é›†

```python
import gzip
import numpy as np

def parse_mnist(image_filename, label_filename):
    # æ‰“å¼€å¹¶è§£å‹ç¼©å›¾åƒæ–‡ä»¶
    with gzip.open(image_filename, 'rb') as image_file:
        # è¯»å–æ–‡ä»¶å¤´ï¼Œè·³è¿‡å‰16å­—èŠ‚
        image_file.read(16)
        # è¯»å–å›¾åƒæ•°æ®
        image_data = image_file.read()

    # æ‰“å¼€å¹¶è§£å‹ç¼©æ ‡ç­¾æ–‡ä»¶
    with gzip.open(label_filename, 'rb') as label_file:
        # è¯»å–æ–‡ä»¶å¤´ï¼Œè·³è¿‡å‰8å­—èŠ‚
        label_file.read(8)
        # è¯»å–æ ‡ç­¾æ•°æ®
        label_data = label_file.read()

    # å°†å›¾åƒå’Œæ ‡ç­¾æ•°æ®è½¬æ¢ä¸ºNumPyæ•°ç»„
    image_array = np.frombuffer(image_data, dtype=np.uint8)
    label_array = np.frombuffer(label_data, dtype=np.uint8)

    # è·å–å›¾åƒçš„ç»´åº¦
    num_images = len(label_array)
    image_dim = len(image_array) // num_images

    # å°†å›¾åƒæ•°æ®è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶å½’ä¸€åŒ–
    image_array = image_array.astype(np.float32) / 255.0

    # è¿”å›å›¾åƒæ•°æ®å’Œæ ‡ç­¾æ•°æ®çš„å…ƒç»„
    return image_array.reshape(num_images, image_dim), label_array

```

## é—®é¢˜3: å®ç°SoftMax Loss

åœ¨ `src/simple_ml.py`æ–‡ä»¶ä¸­å®ç°softmaxæŸå¤±ï¼ˆåˆåäº¤å‰ç†µæŸå¤±ï¼‰å‡½æ•°ï¼Œå³ `softmax_loss()`å‡½æ•°ã€‚å›é¡¾ä¸€ä¸‹ï¼ˆå¸Œæœ›è¿™æ˜¯å¤ä¹ ï¼Œä½†æˆ‘ä»¬ä¹Ÿä¼šåœ¨9æœˆ1æ—¥çš„è¯¾ä¸Šè®²è§£ï¼‰ï¼Œå¯¹äºå¯ä»¥å–å€¼ $y \in \{1,\ldots,k\}$çš„å¤šç±»è¾“å‡ºï¼ŒsoftmaxæŸå¤±å‡½æ•°æ¥å—ä¸€ä¸ªå‘é‡ $z \in \mathbb{R}^k$ä½œä¸ºè¾“å…¥ï¼Œè¿™ä¸ªå‘é‡åŒ…å«äº†å¯¹æ•°æ¦‚ç‡ï¼Œä»¥åŠä¸€ä¸ªçœŸå®çš„ç±»$y \in \{1,\ldots,k\}$ï¼Œè¿”å›å®šä¹‰å¦‚ä¸‹çš„æŸå¤±ï¼š

$$
\ell_{\mathrm{softmax}}(z, y) = \log\sum_{i=1}^k \exp z_i - z_y.
$$

è¯·æ³¨æ„ï¼Œæ­£å¦‚å…¶æ–‡æ¡£å­—ç¬¦ä¸²ä¸­æ‰€æè¿°çš„ï¼Œ`softmax_loss()`æ¥å—ä¸€ä¸ª_äºŒç»´æ•°ç»„_çš„logitsï¼ˆå³ï¼Œä¸€ä¸ªæ‰¹æ¬¡ä¸­ä¸åŒæ ·æœ¬çš„ \( k \) ç»´logitsï¼‰ï¼Œä»¥åŠä¸€ä¸ªç›¸åº”çš„ä¸€ç»´æ•°ç»„çš„çœŸå®æ ‡ç­¾ï¼Œåº”è¯¥è¾“å‡ºæ•´ä¸ªæ‰¹æ¬¡çš„_softmaxæŸå¤±çš„å¹³å‡å€¼_ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†æ­£ç¡®æ‰§è¡Œè¿™ä¸€è®¡ç®—ï¼Œä½ åº”è¯¥_ä¸ä½¿ç”¨ä»»ä½•å¾ªç¯_ï¼Œè€Œæ˜¯å®Œå…¨ä½¿ç”¨numpyçš„å‘é‡åŒ–æ“ä½œï¼ˆä¸ºäº†è®¾å®šæœŸæœ›å€¼ï¼Œæˆ‘ä»¬åº”è¯¥æ³¨æ„åˆ°æˆ‘ä»¬çš„å‚è€ƒè§£å†³æ–¹æ¡ˆç”±ä¸€è¡Œä»£ç ç»„æˆï¼‰ã€‚

è¯·æ³¨æ„ï¼Œå¯¹äºâ€œçœŸå®â€çš„softmaxæŸå¤±å®ç°ï¼Œä½ ä¼šå¸Œæœ›å¯¹logitsè¿›è¡Œç¼©æ”¾ä»¥é˜²æ­¢æ•°å€¼æº¢å‡ºï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬ä¸éœ€è¦æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ï¼ˆå³ä½¿ä½ ä¸è€ƒè™‘è¿™ä¸ªé—®é¢˜ï¼Œå‰©ä½™çš„ä½œä¸šä¹Ÿèƒ½æ­£å¸¸å·¥ä½œï¼‰ã€‚ä¸‹é¢çš„ä»£ç è¿è¡Œæµ‹è¯•ç”¨ä¾‹ã€‚

```Python
def softmax_loss(Z, y):
    """ Return softmax loss.  Note that for the purposes of this assignment,
    you don't need to worry about "nicely" scaling the numerical properties
    of the log-sum-exp computation, but can just compute this directly.

    Args:
        Z (np.ndarray[np.float32]): 2D numpy array of shape
            (batch_size, num_classes), containing the logit predictions for
            each class.
        y (np.ndarray[np.uint8]): 1D numpy array of shape (batch_size, )
            containing the true label of each example.

    Returns:
        Average softmax loss over the sample.
    """
    # ç¡®ä¿è¾“å…¥çš„logits Zæ˜¯æµ®ç‚¹æ•°ï¼Œä»¥ä¾¿è¿›è¡Œexpè®¡ç®—
    Z = Z.astype(np.float64)

    # è®¡ç®—log-sum-expï¼Œå³å¯¹æ¯ä¸ªæ ·æœ¬çš„logitsåº”ç”¨expï¼Œç„¶åæŒ‰è¡Œæ±‚å’Œï¼Œæœ€åå–å¯¹æ•°ã€‚
    # è¿™ä¸€æ­¥ä¼šå¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„log-sum-expå€¼ã€‚
    log_sum_exp = np.log(np.sum(np.exp(Z), axis=1))

    # ä»log-sum-expä¸­å‡å»æ¯ä¸ªæ ·æœ¬çœŸå®ç±»åˆ«çš„logitã€‚
    # np.arange(y.shape[0])ç”Ÿæˆä¸€ä¸ªç´¢å¼•æ•°ç»„ï¼Œç”¨äºé€‰æ‹©æ¯ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«çš„logitã€‚
    correct_logit = Z[np.arange(y.shape[0]), y]

    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„softmaxæŸå¤±
    softmax_loss_per_sample = log_sum_exp - correct_logit

    # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡softmaxæŸå¤±
    average_loss = np.mean(softmax_loss_per_sample)

    return average_loss
```

## é—®é¢˜4ï¼šSoftmaxå›å½’çš„éšæœºæ¢¯åº¦ä¸‹é™

åœ¨è¿™ä¸ªé—®é¢˜ä¸­ï¼Œæ‚¨å°†ä¸º(çº¿æ€§)Softmaxå›å½’å®ç°éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨9æœˆ1æ—¥çš„è¯¾ä¸Šè®¨è®ºçš„ï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¸€ä¸ªå‡è®¾å‡½æ•°ï¼Œå®ƒé€šè¿‡ä¸‹é¢çš„å‡½æ•°å°†$n$ç»´è¾“å…¥è½¬æ¢ä¸º$k$ç»´çš„å¯¹æ•°å‡ ç‡ï¼ˆlogitsï¼‰ï¼š

$$
h(x) = \Theta^T x
$$

å…¶ä¸­ $x \in \mathbb{R}^n$ æ˜¯è¾“å…¥ï¼Œè€Œ $\Theta \in \mathbb{R}^{n \times k}$ æ˜¯æ¨¡å‹å‚æ•°ã€‚ç»™å®šä¸€ä¸ªæ•°æ®é›† $\{(x^{(i)} \in \mathbb{R}^n, y^{(i)} \in \{1,\ldots,k\})\}$ï¼Œå¯¹äº $i=1,\ldots,m$ï¼Œä¸Softmaxå›å½’ç›¸å…³çš„ä¼˜åŒ–é—®é¢˜ç”±ä¸‹é¢ç»™å‡ºï¼š

$$
\DeclareMathOperator*{\minimize}{minimize}
\minimize_{\Theta} \; \frac{1}{m} \sum_{i=1}^m \ell_{\mathrm{softmax}}(\Theta^T x^{(i)}, y^{(i)}).
$$

å›æƒ³ä¸€ä¸‹è¯¾å ‚ä¸Šçš„å†…å®¹ï¼Œçº¿æ€§Softmaxç›®æ ‡çš„æ¢¯åº¦ç”±ä¸‹é¢ç»™å‡ºï¼š

$$
\nabla_\Theta \ell_{\mathrm{softmax}}(\Theta^T x, y) = x (z - e_y)^T
$$

å…¶ä¸­

$$
z = \frac{\exp(\Theta^T x)}{1^T \exp(\Theta^T x)} \equiv \text{normalize}(\exp(\Theta^T x))
$$

ï¼ˆå³ï¼Œ$z$åªæ˜¯å½’ä¸€åŒ–çš„Softmaxæ¦‚ç‡ï¼‰ï¼Œå¹¶ä¸”$e_y$è¡¨ç¤ºç¬¬$y$ä¸ªå•ä½åŸºï¼Œä¹Ÿå°±æ˜¯ï¼Œåœ¨ç¬¬$y$ä¸ªä½ç½®æ˜¯1ï¼Œå…¶ä»–ä½ç½®éƒ½æ˜¯0çš„å‘é‡ã€‚

æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨æˆ‘ä»¬åœ¨è¯¾å ‚ä¸Šè®¨è®ºè¿‡çš„æ›´ç´§å‡‘çš„è¡¨ç¤ºæ³•æ¥è¡¨è¾¾è¿™ä¸€ç‚¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬è®©$X \in \mathbb{R}^{m \times n}$è¡¨ç¤ºæŸ$m$ä¸ªè¾“å…¥çš„è®¾è®¡çŸ©é˜µï¼ˆä¸è®ºæ˜¯æ•´ä¸ªæ•°æ®é›†è¿˜æ˜¯ä¸€ä¸ªå°æ‰¹æ¬¡ï¼‰ï¼Œ$y \in \{1,\ldots,k\}^m$å¯¹åº”çš„æ ‡ç­¾å‘é‡ï¼Œä»¥åŠé‡è½½$\ell_{\mathrm{softmax}}$æ¥æŒ‡ä»£å¹³å‡SoftmaxæŸå¤±ï¼Œé‚£ä¹ˆ

$$
\nabla_\Theta \ell_{\mathrm{softmax}}(X \Theta, y) = \frac{1}{m} X^T (Z - I_y)
$$

å…¶ä¸­

$$
Z = \text{normalize}(\exp(X \Theta)) \quad \text{ï¼ˆå½’ä¸€åŒ–åº”ç”¨äºé€è¡Œï¼‰}
$$

è¡¨ç¤ºå¯¹æ•°å‡ ç‡çŸ©é˜µï¼Œè€Œ$I_y \in \mathbb{R}^{m \times k}$ä»£è¡¨$y$ä¸­æ ‡ç­¾çš„ç‹¬çƒ­ç¼–ç çš„åˆå¹¶ã€‚

ä½¿ç”¨è¿™äº›æ¢¯åº¦ï¼Œå®ç° `softmax_regression_epoch()`å‡½æ•°ï¼Œè¯¥å‡½æ•°ä½¿ç”¨æŒ‡å®šçš„å­¦ä¹ ç‡/æ­¥é•¿ `lr`å’Œå°æ‰¹æ¬¡å¤§å° `batch`è¿è¡ŒSGDçš„å•ä¸ªå‘¨æœŸï¼ˆä¸€æ¬¡æ•°æ®é›†çš„ä¼ é€’ï¼‰ã€‚å¦‚æ–‡æ¡£å­—ç¬¦ä¸²æ‰€æè¿°çš„ï¼Œæ‚¨çš„å‡½æ•°åº”è¯¥å°±åœ°ä¿®æ”¹ `Theta`æ•°ç»„ã€‚å®ç°åï¼Œè¿è¡Œæµ‹è¯•ã€‚

```Python
def softmax_regression_epoch(X, y, theta, lr=0.1, batch=100):
    """ Run a single epoch of SGD for softmax regression on the data, using
    the step size lr and specified batch size.  This function should modify the
    theta matrix in place, and you should iterate through batches in X _without_
    randomizing the order.

    Args:
        X (np.ndarray[np.float32]): 2D input array of size
            (num_examples x input_dim).
        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)
        theta (np.ndarrray[np.float32]): 2D array of softmax regression
            parameters, of shape (input_dim, num_classes)
        lr (float): step size (learning rate) for SGD
        batch (int): size of SGD minibatch

    Returns:
        None
    """
    num_examples = X.shape[0]
    for start_idx in range(0, num_examples, batch):
        end_idx = min(start_idx + batch, num_examples)
        X_batch = X[start_idx:end_idx]
        y_batch = y[start_idx:end_idx]
  
        # Compute logits for the current batch
        logits = np.dot(X_batch, theta)
  
        # Compute softmax gradient
        grad = softmax_grad(logits, y_batch)
  
        # Update the Theta parameters
        theta -= lr * np.dot(X_batch.T, grad)
```

## é—®é¢˜ 5: ä¸¤å±‚ç¥ç»ç½‘ç»œçš„éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰

æ—¢ç„¶æ‚¨å·²ç»ä¸ºçº¿æ€§åˆ†ç±»å™¨ç¼–å†™äº†éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼Œç°åœ¨è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç®€å•çš„ä¸¤å±‚ç¥ç»ç½‘ç»œçš„æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºè¾“å…¥ \( x \in \mathbb{R}^n \)ï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¸€ä¸ªæ²¡æœ‰åç½®é¡¹çš„ä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œå½¢å¼å¦‚ä¸‹ï¼š

$$
z = W_2^T \mathrm{ReLU}(W_1^T x)
$$

è¿™é‡Œ \( W_1 \in \mathbb{R}^{n \times d} \) å’Œ \( W_2 \in \mathbb{R}^{d \times k} \) è¡¨ç¤ºç½‘ç»œçš„æƒé‡ï¼ˆç½‘ç»œå…·æœ‰ä¸€ä¸ª \( d \) ç»´çš„éšè—å•å…ƒï¼‰ï¼Œè€Œ \( z \in \mathbb{R}^k \) è¡¨ç¤ºç½‘ç»œè¾“å‡ºçš„logitsã€‚æˆ‘ä»¬å†æ¬¡ä½¿ç”¨softmax/äº¤å‰ç†µæŸå¤±ï¼Œæ„å‘³ç€æˆ‘ä»¬æƒ³è¦è§£å†³ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ï¼š

$$
\minimize_{W_1, W_2} \;\; \frac{1}{m} \sum_{i=1}^m \ell_{\mathrm{softmax}}(W_2^T \mathrm{ReLU}(W_1^T x^{(i)}), y^{(i)}).
$$

æˆ–è€…ï¼Œä½¿ç”¨çŸ©é˜µ \( X \in \mathbb{R}^{m \times n} \) æ¥æè¿°æ‰¹é‡å½¢å¼çš„æƒ…å†µï¼Œä¹Ÿå¯ä»¥å†™æˆï¼š

$$
\minimize_{W_1, W_2} \;\; \ell_{\mathrm{softmax}}(\mathrm{ReLU}(X W_1) W_2, y).
$$

ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡ºè¿™ä¸ªç½‘ç»œçš„åå‘ä¼ æ’­æ›´æ–°ï¼ˆæˆ‘ä»¬å°†åœ¨9/8çš„è¯¾ä¸Šç®€è¦ä»‹ç»ï¼Œä½†ä¸ºäº†ä¾¿äºå®ç°ï¼Œè¿™é‡Œä¹Ÿæä¾›äº†æœ€ç»ˆå½¢å¼ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾ï¼š

$$
\begin{split}
Z_1 \in \mathbb{R}^{m \times d} & = \mathrm{ReLU}(X W_1) \\
G_2 \in \mathbb{R}^{m \times k} & = \normalize(\exp(Z_1 W_2)) - I_y \\
G_1 \in \mathbb{R}^{m \times d} & = \mathrm{1}\{Z_1 > 0\} \circ (G_2 W_2^T)
\end{split}
$$

è¿™é‡Œ \( \mathrm{1}\{Z_1 > 0\} \) æ˜¯ä¸€ä¸ªäºŒå…ƒçŸ©é˜µï¼Œå…¶å…ƒç´ ç­‰äºé›¶æˆ–ä¸€ï¼Œè¿™å–å†³äº \( Z_1 \) ä¸­çš„æ¯ä¸€é¡¹æ˜¯å¦ä¸¥æ ¼ä¸ºæ­£ï¼Œä¸” \( \circ \) è¡¨ç¤ºæŒ‰å…ƒç´ ä¹˜æ³•ã€‚é‚£ä¹ˆç›®æ ‡çš„æ¢¯åº¦ç”±ä¸‹å¼ç»™å‡ºï¼š

$$
\begin{split}
\nabla_{W_1} \ell_{\mathrm{softmax}}(\mathrm{ReLU}(X W_1) W_2, y) & = \frac{1}{m} X^T G_1  \\
\nabla_{W_2} \ell_{\mathrm{softmax}}(\mathrm{ReLU}(X W_1) W_2, y) & = \frac{1}{m} Z_1^T G_2.  \\
\end{split}
$$

ä½¿ç”¨è¿™äº›æ¢¯åº¦ï¼Œç°åœ¨ç¼–å†™ `src/simple_ml.py` æ–‡ä»¶ä¸­çš„ `nn_epoch()` å‡½æ•°ã€‚ä¸å‰ä¸€ä¸ªé—®é¢˜ä¸€æ ·ï¼Œæ‚¨çš„è§£å†³æ–¹æ¡ˆåº”è¯¥å°±åœ°ä¿®æ”¹ `W1` å’Œ `W2` æ•°ç»„ã€‚å®ç°å‡½æ•°åï¼Œè¿è¡Œä»¥ä¸‹æµ‹è¯•ã€‚è¯·åŠ¡å¿…ä½¿ç”¨ä¸Šè¿°è¡¨è¾¾å¼æ‰€æŒ‡ç¤ºçš„çŸ©é˜µæ“ä½œæ¥å®ç°å‡½æ•°ï¼šè¿™å°†æ¯”å°è¯•ä½¿ç”¨å¾ªç¯_å¿«å¾—å¤š_ï¼Œè€Œä¸”æ•ˆç‡æ›´é«˜ï¼ˆè€Œä¸”éœ€è¦çš„ä»£ç è¿œè¿œå°‘ï¼‰ã€‚

åœ¨ä¸Šè¿°çš„è¯¾ç¨‹æè¿°ä¸­ï¼Œå·²ç»éå¸¸å®Œå¤‡çš„ç»™å‡ºäº†å¯¹åº”çš„å…¬å¼ï¼Œå¯¹ç…§ç€è¿›è¡Œå®ç°å³å¯ã€‚

```Python
def nn_epoch(X, y, W1, W2, lr=0.1, batch=100):
    """ Run a single epoch of SGD for a two-layer neural network defined by the
    weights W1 and W2 (with no bias terms):
        logits = ReLU(X * W1) * W2
    The function should use the step size lr, and the specified batch size (and
    again, without randomizing the order of X).  It should modify the
    W1 and W2 matrices in place.

    Args:
        X (np.ndarray[np.float32]): 2D input array of size
            (num_examples x input_dim).
        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)
        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape
            (input_dim, hidden_dim)
        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape
            (hidden_dim, num_classes)
        lr (float): step size (learning rate) for SGD
        batch (int): size of SGD minibatch

    Returns:
        None
    """
    num_examples = X.shape[0]
    num_classes = W2.shape[1]
  
    # Convert y to one-hot encoding
    y_one_hot = np.eye(num_classes)[y]
  
    for start_idx in range(0, num_examples, batch):
        end_idx = min(start_idx + batch, num_examples)
        X_batch = X[start_idx:end_idx]
        y_batch = y_one_hot[start_idx:end_idx]
  
        # Forward pass
        Z1 = ReLU(X_batch @ W1)  # Activation from first layer
        logits = Z1 @ W2  # Logits for the current batch
        probabilities = softmax(logits)
  
        # Compute the gradients for softmax loss
        G2 = probabilities - y_batch  # Gradient for softmax/cross-entropy layer
        G1 = (Z1 > 0).astype(float) * (G2 @ W2.T)  # Gradient for ReLU layer
  
        # Compute the gradients for weights
        grad_W1 = (X_batch.T @ G1) / batch
        grad_W2 = (Z1.T @ G2) / batch
  
        # Update the weights
        W1 -= lr * grad_W1
        W2 -= lr * grad_W2
```

## é—®é¢˜6: ç”¨C++å®ç°Softmax Regression

ä½¿ç”¨C++é‡å†™ä¸€éé—®é¢˜4ã€‚ç”±äºä½¿ç”¨çš„æ˜¯åŸç”Ÿçš„C++ï¼Œå› æ­¤æœ‰å¾ˆå¤šçš„ä»£ç éœ€è¦é‡æ–°å†™ï¼Œä¸‹é¢æ˜¯é¢˜ç›®å‡ºç»™å‡ºçš„ä»£ç ã€‚

```cpp
void softmax_regression_epoch_cpp(const float *X, const unsigned char *y, 
								  float *theta, size_t m, size_t n, size_t k, 
								  float lr, size_t batch)
{
    /**
     * A C++ version of the softmax regression epoch code.  This should run a 
     * single epoch over the data defined by X and y (and sizes m,n,k), and
     * modify theta in place.  Your function will probably want to allocate
     * (and then delete) some helper arrays to store the logits and gradients.
     * 
     * Args:
     *     X (const float *): pointer to X data, of size m*n, stored in row 
     *          major (C) format
     *     y (const unsigned char *): pointer to y data, of size m
     *     theta (float *): pointer to theta data, of size n*k, stored in row
     *          major (C) format
     *     m (size_t): number of examples
     *     n (size_t): input dimension
     *     k (size_t): number of classes
     *     lr (float): learning rate / SGD step size
     *     batch (int): SGD minibatch size
     * 
     * Returns:
     *     (None)
     */

    /// YOUR CODE HERE
  
    /// END YOUR CODE
}
```

æ ¹æ®é—®é¢˜å››çš„æ€è·¯ï¼Œä¸»è¦æ­¥éª¤åˆ†ä¸ºä¸‰æ­¥ï¼š

1. æ ¹æ®å°æ‰¹é‡ï¼Œè®¡ç®—å¾—åˆ°logits
2. è®¡ç®—æ¢¯åº¦
3. æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°

å› æ­¤æˆ‘ä»¬åº”è¯¥å¯ä»¥å†™å‡ºå¦‚ä¸‹ä»£ç æ¡†æ¶

```cpp
// å®šä¹‰softmaxå›å½’è®­ç»ƒè¿­ä»£çš„å‡½æ•°
// è¾“å…¥å‚æ•°åŒ…æ‹¬ï¼šç‰¹å¾çŸ©é˜µXï¼Œç›®æ ‡å€¼æ•°ç»„yï¼Œå‚æ•°çŸ©é˜µthetaï¼Œæ ·æœ¬æ•°é‡mï¼Œç‰¹å¾æ•°é‡nï¼Œç±»åˆ«æ•°é‡kï¼Œ
// å­¦ä¹ ç‡lrï¼Œä»¥åŠæ¯æ‰¹å¤„ç†çš„æ ·æœ¬æ•°é‡batch
void softmax_regression_epoch_cpp(const float *X, const unsigned char *y,
                                  float *theta, size_t m, size_t n, size_t k,
                                  float lr, size_t batch)
{
    // ä¸ºlogitsï¼ˆé€»è¾‘å‡½æ•°çš„è¾“å‡ºï¼‰å’Œæ¢¯åº¦åˆ†é…å†…å­˜ç©ºé—´
    std::vector<float> logits(batch * k);
    std::vector<float> gradients(batch * k);

    // é€šè¿‡è¿­ä»£å¤„ç†æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬
    for (size_t i = 0; i < m; i += batch) {
        // ç¡®å®šå½“å‰æ‰¹æ¬¡çš„å®é™…å¤§å°ï¼ˆå¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡æ—¶å¯èƒ½ä¸æ»¡ï¼‰
        size_t current_batch_size = std::min(batch, m - i);

        // è®¡ç®—å½“å‰æ‰¹æ¬¡çš„logitsã€‚'X + i * n'å®šä½åˆ°å½“å‰æ‰¹æ¬¡çš„ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾æ•°æ®èµ·å§‹ç‚¹ã€‚
        dot_product(X + i * n, theta, logits.data(), current_batch_size, n, k);

        // å°†åŸå§‹logitsæ•°ç»„çš„å†…å®¹å¤åˆ¶åˆ°std::vectorä¸­ï¼Œä»¥æ–¹ä¾¿åç»­å¤„ç†
        std::vector<float> logits_vector(logits.data(), logits.data() + current_batch_size * k);
  
        // ä¸ºå½“å‰æ‰¹æ¬¡åˆå§‹åŒ–æ¢¯åº¦å‘é‡
        std::vector<float> gradient_vector(current_batch_size * k);

        // è°ƒç”¨softmax_gradå‡½æ•°è®¡ç®—æ¢¯åº¦
        softmax_grad(logits_vector, std::vector<unsigned char>(y + i, y + i + current_batch_size),
                     current_batch_size, k, gradient_vector);

        // æ›´æ–°thetaå‚æ•°çŸ©é˜µ
        // å¯¹äºå½“å‰æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼ˆç”±å¤–å¾ªç¯jæ§åˆ¶ï¼‰
        for (size_t j = 0; j < current_batch_size; ++j) {
            // å¯¹äºæ¯ä¸ªç±»åˆ«ï¼ˆç”±ä¸­å¾ªç¯cæ§åˆ¶ï¼‰
            for (size_t c = 0; c < k; ++c) {
                // å¯¹äºæ¯ä¸ªç‰¹å¾ï¼ˆç”±å†…å¾ªç¯dæ§åˆ¶ï¼‰
                for (size_t d = 0; d < n; ++d) {
                    // 'theta[d * k + c]'æ˜¯thetaçŸ©é˜µä¸­å¯¹åº”äºç¬¬dä¸ªç‰¹å¾å¯¹ç¬¬cä¸ªç±»åˆ«çš„æƒé‡
                    // å®ƒæ˜¯ä¸€ç»´æ•°ç»„ä¸­çš„ç´¢å¼•ï¼Œä½†ä»£è¡¨äºŒç»´çŸ©é˜µçš„ä½ç½®ï¼ˆdè¡Œcåˆ—ï¼‰
                    // 'X[(i + j) * n + d]'æ˜¯å½“å‰æ‰¹æ¬¡ä¸­ç¬¬jä¸ªæ ·æœ¬çš„ç¬¬dä¸ªç‰¹å¾å€¼
                    // è¿™ä¸ªå€¼ä¸å¯¹åº”çš„æ¢¯åº¦å’Œå­¦ä¹ ç‡ç›¸ä¹˜åï¼Œç”¨äºæ›´æ–°thetaçŸ©é˜µçš„å¯¹åº”æƒé‡
                    // è¿™é‡Œè¿›è¡Œçš„æ“ä½œæ˜¯æ¢¯åº¦ä¸‹é™æ­¥éª¤ï¼Œç”¨äºä¼˜åŒ–æŸå¤±å‡½æ•°
                    theta[d * k + c] -= lr * gradient_vector[j * k + c] * X[(i + j) * n + d];
                }
            }
        }
    }
}

```

è¿™ä¸ªæ—¶å€™åªéœ€è¦å®Œæˆä¸¤ä¸ªå‡½æ•°dot_productã€softmax_gradè¿™ä¸¤ä¸ªå‡½æ•°çš„å®ç°å³å¯ã€‚ä¸‹é¢ç»™å‡ºå‚è€ƒçš„å®ç°ï¼š

```cpp
void dot_product(const float* A, const float* B, float* C, size_t A_rows, size_t A_cols, size_t B_cols) {
    // Initialize C with zeros
    std::fill(C, C + A_rows * B_cols, 0.0f);

    // Compute the dot product
    for (size_t i = 0; i < A_rows; ++i) {            // Iterate over the rows of A
        for (size_t j = 0; j < B_cols; ++j) {        // Iterate over the columns of B
            for (size_t k = 0; k < A_cols; ++k) {    // Dot product calculation
                C[i * B_cols + j] += A[i * A_cols + k] * B[k * B_cols + j];
            }
        }
    }
}
```

```cpp
// Helper function to compute the softmax probabilities for a vector
std::vector<float> softmax(const std::vector<float>& logits) {
    std::vector<float> probabilities(logits.size());
    float max_logit = *std::max_element(logits.begin(), logits.end());
    float sum_exp = 0.0;

    for (size_t i = 0; i < logits.size(); ++i) {
        probabilities[i] = std::exp(logits[i] - max_logit);
        sum_exp += probabilities[i];
    }

    for (size_t i = 0; i < logits.size(); ++i) {
        probabilities[i] /= sum_exp;
    }

    return probabilities;
}

// Function to compute the gradient of the softmax loss
void softmax_grad(const std::vector<float>& Z, const std::vector<unsigned char>& y,
                  size_t batch_size, size_t num_classes, std::vector<float>& gradient) {
    std::vector<float> probabilities;

    // Compute softmax probabilities
    for (size_t i = 0; i < batch_size; ++i) {
        std::vector<float> logits(Z.begin() + i * num_classes, Z.begin() + (i + 1) * num_classes);
        std::vector<float> probs = softmax(logits);

        // Subtract 1 from the probability of the correct class
        probs[y[i]] -= 1;

        // Copy the probabilities back into the gradient vector
        std::copy(probs.begin(), probs.end(), gradient.begin() + i * num_classes);
    }

    // Average the gradient over the batch
    for (size_t i = 0; i < gradient.size(); ++i) {
        gradient[i] /= static_cast<float>(batch_size);
    }
}
```

æœ€åè¿è¡Œæµ‹è¯•å‡½æ•°

![i](../img/lab0/f6.png)

## æ€»ç»“

Lab0æ˜¯ä¸»è¦æ˜¯ç”¨æ¥åšè‡ªæ£€çš„ï¼Œä»åŠ è½½æ•°æ®ã€å®ç°lossã€å®ç°æ¢¯åº¦æ›´æ–°ç­‰æ–¹é¢ç»™åˆ°äº†ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨ï¼Œç›¸å…³çš„æç¤ºéƒ½éå¸¸åˆ°ä½ã€‚

ä½†ç¬”è€…å¯¹c++å¿«å¿˜å®Œäº†ï¼Œé—®é¢˜6è¿˜æ˜¯åšèµ·æ¥æ¯”è¾ƒè´¹åŠ²çš„ğŸ˜…
